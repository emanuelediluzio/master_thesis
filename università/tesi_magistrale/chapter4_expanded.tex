\chapter{Implementation}
\label{chap:implementazione}

This chapter details the engineering realization of the SVG captioning system. I describe the software architecture, the specific libraries and tools employed, and the implementation of the core components: the data loading pipeline, the LoRA fine-tuning workflow, and the SVG Path Embedder (SPE). I also discuss the technical challenges encountered--such as gradient stability and context window management--and the solutions adopted.

\section{System Architecture and Technology Stack}
\label{sec:system-architecture}

The system is implemented in Python 3.10 using the PyTorch deep learning framework. The architecture is modular, separating data processing, model definition, and training logic. The data pipeline is responsible for transforming raw SVG files into the hybrid visual-text sequences required by the model.

\subsection{Dataset Construction and Preprocessing}
The dataset used for this thesis is derived from the \textbf{Icons8} collection, as introduced in Chapter 1. This large-scale repository of Scalable Vector Graphics has been curated for machine learning tasks. The raw dataset was organized into \textbf{70 compressed chunks} (JSON files), containing approximately \textbf{700,000 unique SVG files}. 

To process this massive collection efficiently, I implemented a two-stage pipeline.
\begin{enumerate}
    \item \textbf{Conversion}: A custom conversion script transforms the raw JSON chunks into the JSONL format required by the training loop. This step maps the raw data to the specific schema \texttt{\{"input": <svg>..., "output": <caption>\}}, creating a clean, caption-aligned corpus.
    \item \textbf{Splitting}: A dedicated splitting script performs a deterministic, stratified split of the data. Instead of a simple percentage split, it uses a fixed test set size (defaulting to 1000 samples) to ensure a consistent evaluation benchmark across different experiments.
\end{enumerate}
The final processed dataset resulted in a clean, caption-aligned corpus ready for tokenization.

\subsection{SVG Parsing and Tokenization}
\label{sec:svg-parsing}
The parsing logic, implemented in a dedicated preprocessing module, functions as the bridge between raw XML text and the numerical input required by the neural network. Unlike raster image processors that read pixel buffers, this module must interpret the structured syntax of the SVG standard. I adopted a robust regular expression (Regex) strategy to clean and extract path data, avoiding the overhead of full DOM parsing libraries while maintaining high speed.

The \texttt{extract\_paths\_from\_svg} method executes a multi-stage pipeline:
\begin{enumerate}
    \item \textbf{XML Cleaning and Sanitization}: The raw SVG string is first stripped of XML headers, namespaces, metadata tags (like \texttt{<metadata>}, \texttt{<defs>}), and non-geometric elements (\textit{e.g.}, \texttt{<title>}, \texttt{<desc>}). Use of background rectangles (often usually white fills) is also detected and removed to focus the model solely on the semantic content of the icon.
    \item \textbf{Path Data Extraction}: The script scans for \texttt{<path>} tags and extracts the \texttt{d} attribute, which contains the drawing commands. It simultaneously parses the \texttt{style} attributes (fill, stroke, stroke-width) to optionally retain color and stroke information, though the primary focus of this thesis remains on the geometry.
    \item \textbf{Path Extraction}: The script extracts the raw strings from the \texttt{d} attributes. Note that the actual \textbf{tokenization}--breaking down the path strings into discrete symbols (commands M, L, C and coordinates)--is delegated to the LLM's tokenizer (Byte-Pair Encoding or similar) during the training phase, rather than being performed by the preprocessor's regex engine.
    \item \textbf{Sequence Construction}: The extracted paths are concatenated into a single flat sequence of tokens. To manage varying complexities, the system employs two length management strategies:
    \begin{itemize}
        \item \textit{Fixed Length}: Truncates or pads sequences to a strictly defined context length (\textit{e.g.}, 1024 tokens) using a special \texttt{<pad>} token.
        \item \textit{Variable Length}: Wraps the sequence in \texttt{<start\_svg>} and \texttt{<end\_svg>} tokens, allowing the model to attend to the data without artificial padding constraints during inference.
    \end{itemize}
\end{enumerate}
This text-based preprocessing approach effectively treats SVG paths as discrete sequences of symbols, priming them for the embedding layer without the need for complex geometric normalization or rendering steps.

\subsection{Dynamic Padding and Collation}

Since SVGs have variable numbers of elements, I use dynamic padding. The custom collation function in the DataLoader first finds the maximum sequence length $L_{\text{max}}$ in the batch, then pads shorter sequences with a special padding token (encoded as all-zeros), and finally creates a boolean attention mask (1 for valid elements, 0 for padding) to ensure the model ignores padding tokens. This dynamic approach is critical for efficiency. A naive implementation might pad all sequences to the global maximum length (\textit{e.g.}, 512), resulting in a tensor mostly filled with zeros if the batch consists of simple icons. By padding only to the longest sequence \textit{in the current batch}, we significantly reduce the number of computations in the self-attention mechanism, which scales quadratically with sequence length. This optimization reduced training time by approximately 40\% compared to static padding.

\section{The SVG Path Embedder (SPE)}
\label{sec:spe-implementation}

The \textbf{SVG Path Embedder (SPE)} implementation was kindly provided by Leonardo Zini \cite{zini2025spe_autoencoder}. It is integrated as a standalone PyTorch module (\texttt{SVGPathEmbedder} wrapper around the core \texttt{PathEncoder}). Instead of operating on continuous coordinates, the implementation treats SVG path commands and coordinates as a sequence of discrete tokens, leveraging a learnable embedding layer to capture their semantic relationships.

The core component is the \texttt{PathEncoder}, which transforms the tokenized input into dense vector representations:

\begin{enumerate}
    \item \textbf{Input}: A standard batch of integer indices $x \in \mathbb{R}^{B \times L}$ representing the tokenized SVG paths.
    \item \textbf{Discrete Embedding}: The input indices are passed through a learnable Lookup Table (Embedding Layer) of size $V \times D$, where $V$ is the vocabulary size (commands + coordinates treated as text tokens) and $D$ is the embedding dimension ($d_{model}$). This maps each discrete path token to a continuous vector space: $E = \text{Embed}(x)$.
    \item \textbf{Positional Encoding}: To preserve the sequential order of drawing commands, sinusoidal Positional Encodings (PE) are added to the embeddings. This ensures the model distinguishes between the start and end of a curve.
    \item \textbf{Transformer Encoder}: The position-aware embeddings are processed by a standard Transformer Encoder stack (consisting of Self-Attention and Feed-Forward layers). This is the core of the Auto-Encoder provided by Zini \textit{et al.}, which allows the model to learn geometric dependencies.
    \item \textbf{Output}: The final output is a sequence of context-rich vectors ready for projection into the LLM's latent space.
\end{enumerate}

This architecture aligns with the discrete nature of the tokenized SVG input. The \texttt{embedding\_head} maps each unique token (command or quantized coordinate) to a high-dimensional vector in $\mathbb{R}^{d_{model}}$. These embeddings are then enriched with positional information via standard trigonometric position encodings (\texttt{PositionalEncoding}) before being processed by a stack of Transformer encoder layers. This design allows the model to learn context-aware representations of path sequences similar to how LLMs process text.



\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/spe_architecture.png}
    \caption{Architecture of the SVG Path Embedder (SPE). discrete path tokens are mapped to embeddings, enriched with Position Encodings, and processed by a Transformer Encoder.}
    \label{fig:spe_architecture}
\end{figure}

\subsection{The SPE Module}



The \texttt{SVGPathEmbedder} module aggregates the components defined in the methodology. Its forward pass follows a streamlined architecture: (i) \textbf{Discrete Embedding}, where the integer token indices are mapped to dense vectors via a standard lookup table; (ii) \textbf{Positional Encoding}, where sinusoidal information is added to preserve sequence order; and (iii) \textbf{Transformer Encoding}, where the stack of self-attention layers processes the sequence to model geometric dependencies. This implementation avoids complex attribute-specific branches, relying instead on the generic power of the Transformer to learn the syntax of SVG commands.

\section{LoRA Integration and Training Loop}
\label{sec:lora-implementation}

\subsection{Loading the Pre-trained LLM}



I load the pre-trained models (Qwen2-7B, Gemma-9B, Llama-8B) using the Hugging Face \texttt{AutoModelForCausalLM} interface. To optimize memory usage on the available hardware, I utilize \texttt{bfloat16} precision, which offers a wider dynamic range than standard \texttt{float16} and improves training stability.

\subsection{Injecting LoRA Adapters}



I configure the LoRA adapters using the \texttt{peft} library with efficient parameters derived from the stable training configuration: a \textbf{Rank ($r$)} of 16, an \textbf{Alpha ($\alpha$)} of 32, and a \textbf{Dropout} of 0.1. To optimize performance, the base model is loaded in standard precision (\texttt{fp16}), avoiding the latency overhead of 4-bit quantization. The \textbf{Target Modules} include the query, key, value, and output attention projections. This efficient setup allows fine-tuning 7B+ parameter models on consumer-grade hardware.

\subsection{Baseline Training Process (LoRA-LLM)}
For the text-only baseline, the training process follows a standard Causal Language Modeling (CLM) approach. First, (i) \textbf{Input Processing} concatenates the SVG code and caption into a single string, tokenized by the LLM's tokenizer. Second, (ii) \textbf{Forward Pass} feeds token IDs directly to the LLM, updating LoRA adapters to minimize next-token prediction loss. Finally, (iii) \textbf{Objective} ensures the model learns to predict caption tokens based on the preceding SVG text tokens.

\subsection{Multimodal Training Process (SPE-LoRA)}
For the proposed multimodal method (SPE + LLM), the training loop is significantly more complex than the baseline. The forward pass establishes a bridge between the visual encoder and the language model through the following steps:

\begin{enumerate}
    \item \textbf{Visual Encoding}: The raw SVG tokens $s$ are passed through the \textbf{SPE} module. This results in a sequence of visual embeddings $V = [v_1, v_2, \dots, v_N]$, where $v_i \in \mathbb{R}^{d_{model}}$. These vectors essentially "translate" the geometric language of SVGs into the semantic space of the LLM.
    
    \item \textbf{Text Encoding}: The target caption $C$ is tokenized by the LLM's standard tokenizer and embedded into text vectors $T = [t_1, t_2, \dots, t_M]$.
    
    \item \textbf{Embedding Concatenation}: The system constructs the multimodal input sequence by concatenating the visual vectors and text vectors along the sequence dimension. The resulting tensor $Z = [V; T]$ has a total length of $N + M$. Crucially, the LLM's positional encoding (\textit{e.g.}, RoPE) is applied to this entire concatenated sequence, effectively treating the visual and textual tokens as a continuous stream where the image acts as a prefix.
    
    \item \textbf{Attention Mask Construction}: A critical step is creating the combined attention mask. The visual tokens must be fully visible to the text tokens, but the padding in the visual part must be masked out. The mask is constructed as:
    \[
    M_{combined} = [M_{visual}, M_{text}]
    \]
    where $M_{visual}$ ensures the LLM attends to the valid path tokens.
    
    \item \textbf{LoRA-adapted Forward Pass}: The concatenated embeddings $Z$ and the combined mask are fed into the LLM backbone. The LoRA adapters, injected into the Attention layers, learn to modulate the information flow, allowing the pre-trained weights to process the new visual signals without catastrophic forgetting.
    
    \item \textbf{Loss Calculation}: The standard Cross-Entropy Loss is computed only on the text portion of the sequence ($t_1 \dots t_M$), ensuring the model learns to \textit{generate} the caption based on the visual prefix, rather than trying to reconstruct the SVG itself.
\end{enumerate}

\subsubsection{Visual-Semantic Metric (Primary)}
To assess visual grounding without relying on reference text, I use \textbf{CLIPScore} \cite{hessel2021clipscore}. We consider \textbf{CLIPScore} as the \textbf{primary metric} for this evaluation, as it directly measures the semantic alignment between the generated caption and the visual content of the \textit{rasterized} SVG. Since the goal is to describe "what is seen", this metric correlates best with human utility. It computes the cosine similarity between the caption embedding and the image embedding using a pre-trained CLIP model (ViT-B/32).

\section{Challenges and Solutions}
\label{sec:challenges}

\subsection{Addressing Gradient Explosion}
A significant technical challenge encountered during the early training phases was \textbf{gradient explosion}. This phenomenon manifested as a sudden spike in the loss function (divergence) followed by the model generating nonsense output.

\textbf{Root Cause Analysis}: The issue stemmed from the initialization variance of the \textbf{SVG Path Embedder (SPE)}. The discrete embedding layer and the subsequent transformer encoder were initialized with standard distributions, producing output vectors with a magnitude and variance significantly different from the pre-trained, well-normalized embedding space of the LLM. This "magnitude mismatch" caused the projection layer to generate large gradients in an attempt to bridge the two latent spaces, destabilizing the update steps for the LoRA adapters.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/vanishing_exploding_gradient.png}
    \caption{Illustration of the Vanishing and Exploding Gradient problem. In deep networks, gradients can grow exponentially (exploding) or decay (vanishing) as they backpropagate through layers, leading to unstable training. Proper initialization and techniques like Gradient Clipping are essential to mitigate these issues.}
    \label{fig:gradient_explosion}
\end{figure}

\textbf{Solution}: I implemented two key stabilizations in the training loop: (i) \textbf{Gradient Clipping}, applying a maximum norm of 1.0 to model parameters before optimization to cap gradient magnitude; and (ii) \textbf{Layer Normalization}, adding a \texttt{LayerNorm} after the SPE's output projection to normalize visual embedding statistics to zero mean and unit variance, aligning them with the Transformer backbone. This combination proved decisive: without it, training diverged within the first 50 steps; with it, the loss curve showed a smooth, monotonic decrease.


\subsection{Hyperparameters and Training Details}
\label{sec:hyperparameters}

To ensure reproducibility, I report the exact hyperparameters used for the stable training runs, verified against the saved model configuration on the server. The training was performed using the \textbf{AdamW} optimizer (standard PyTorch implementation) with $\beta_1=0.9$, $\beta_2=0.999$, weight decay of $\lambda=0.01$, and $\epsilon=10^{-8}$. To prevent gradient explosion, I applied \textbf{Gradient Clipping} with a maximum norm of 1.0.

The \textbf{Learning Rate} was set to $2 \times 10^{-5}$ utilizing a cosine scheduler with restarts. Specific scheduler parameters were set to $T_0=1000$ steps (initial cycle length), $T_{mult}=2$ (cycle expansion factor), and a minimum learning rate $\eta_{min}=10^{-6}$, ensuring the model could escape local minima during the \textbf{5 training epochs}.

To stabilize the training dynamics, we employed \textbf{Global Gradient Norm Clipping} \cite{pascanu2013gradients}. Unlike value clipping (which clips individual gradients element-wise), norm clipping rescales the entire gradient vector $g$ (where $g$ represents the concatenation of gradients for all trainable parameters) if its global $L_2$ norm $||g||_2$ exceeds a threshold $\theta$. The update rule is:
\[ g \leftarrow g \cdot \frac{\theta}{\max(||g||_2, \theta)} \]
This ensures that the direction of the gradient descent update helps is preserved, while the step size is effectively capped. This was critical for the hybrid architecture: the unnormalized gradients from the initialized SPE were initially magnitudes larger than those of the pre-trained LLM, leading to instability. We set $\theta=1.0$ for Qwen2 based on empirical stability, while tighter constraints ($\theta=0.3$) were necessary for Llama and Gemma to prevent divergence in mixed-precision training.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/wandb_training.png}
    \caption{Training loss, evaluation loss, and system metrics (GPU utilization) during the fine-tuning of the SPE + Qwen2-7B model.}
    \label{fig:training_metrics}
\end{figure}



