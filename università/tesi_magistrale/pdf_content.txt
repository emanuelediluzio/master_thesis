LLM-BASED SVG IMAGE CAPTIONING WITH
CONCEPTUAL EMBEDDINGS

Emanuele Di Luzio

Advisors

337543@studenti.unimore.it

Prof. Lorenzo Baraldi
Dott. Leonardo Zini

Index

1.Introduction: SVG introduction & Objective Definition.
2.Problem Statement: Pixels vs. Primitives.
3.Methodology I: The Decoder-Only Backbone.
4.Methodology II: The SPE + Decoder Architecture.
5.Implementation: Dataset & Training Strategies.
6.Results: Quantitative and Qualitative Analysis.
7.Conclusions: Future Directions.

The World is Built on Vectors
Key Points:

Ubiquity: Icons, Logos, UI elements, Technical
Diagrams.
Advantage: Infinite Scalability (ResolutionIndependent).
Structure: SVG (Scalable Vector Graphics) is XMLbased textual markup.
The Primitive: The <path> tag is the core building
block.
M (x,y): Move to coordinates.
L (x,y): Draw Line to.
C (x1,y1, x2,y2, x,y): Cubic Bezier Curve.
Graphics as Code: It is not a grid of pixels; it is a
sequence of mathematical instructions.

The Task: SVG Image Captioning

Goal: Automatically generate textual descriptions for vector graphics.
Why It Matters:
Accessibility: Screen readers need alt-text for icons.
Search & Retrieval: Enable semantic search in icon libraries.
Automation: Scale description generation for millions of assets.
The Challenge:
Traditional VLMs (BLIP-2, LLaVA) require rasterization pixel grids.
Destroys structural information, wastes compute on background pixels.
Our Solution:
Skip rasterization. Feed SVG code directly to an LLM.
Use a specialized encoder (SPE) to convert geometry into embeddings.

→

The Core Challenge: Pixels vs. Primitives

The Inefficiency of
Rasterization

The Vector-Native
Opportunity:

Information Loss: Converting XML to
Pixels is irreversible (SVG to Image).

Semantic: We treat SVG as Code (XML
is a language).

Structure Lost: Semantic relationships
between shapes are destroyed.

Continuous: No resolution artifacts. A
circle is a mathematical curve, not a grid
of dots.

Compute Waste: A simple diagonal line
requires processing thousands of white
background pixels.

Efficiency: Dense semantic signal (few
tokens) vs sparse pixel signal (many
patches).

Methodology I: The Decoder-Only Backbone
Why Decoder-Only?:
Causal Self-Attention: Trains the model to predict the next token based on context.
Generative Power: Superior at producing fluent, coherent natural language compared to EncoderDecoders.
The model already knows what a "circle" or "arrow" is conceptually.
Our goal is to align visual features to these pre-existing linguistic concepts.

Model Selection Strategy:
Qwen2-7B: Chosen for strictly better Reasoning & Coding capabilities (crucial for structured
XML data).
Gemma-9B: Google's efficient open model, strong on general knowledge.
Llama-3-8B: Meta's model, selected for its Superior Linguistic Quality (Best-in-class text
generation and fluency).

Methodology I: Text-Only Fine-tuning (LoRA)

Approach: Treat SVG code purely as text, the Decoder receives the SVG exactly like a sentence,
processing drawing commands (M, L, C) sequentially from start to finish.(we preprocess it by stripping
non-geometric elements like metadata and background fills, then let the LLM tokenize the clean path
commands.) Zero-shot LLMs on raw SVG failed completely (incoherent outputs, XML parsing confusion).
The Technique: Low-Rank Adaptation (LoRA).
Instead of full retraining, we inject small matrices (A times B) into the LLM.
Allows efficient adaptation to "SVG Language" with <1% params.
Result: High CLIPScore (32.30) but hallucinates content, proving that standard metrics can be
misleading without geometric grounding.

Methodology II: The SPE + Decoder Architecture
The Missing Piece: Explicit Geometric Understanding.

Our Architecture:
Input: Visual Embeddings from SPE (Frozen).
Bridge: Projection Layer (The "Middle Layer") connects SPE to LLM.
Backbone: The LoRA-adapted LLM from the previous step.

Mechanism:
The LLM now attends to both Visual Tokens (Geometry) and Text Tokens (Caption).
SPE (SVG Path Embedder) isn't just a random encoder;
it is a pre-trained Auto-Encoder that has already learned to reconstruct SVG paths. We
insert a Projection Layer (SPE Output (384-dim) nn.Linear(384, 4096) LayerNorm
LLM Input (4096-dim)) to align its dense, pre-learned features with the LLM's embedding
space. Now, the model doesn't just read code, it 'sees' the geometry through an expert
eye.

→

→

→

Dataset Construction

Source: Icons8 (Consistent, high-quality style).
Size: ~90,000 SVG-Caption pairs.
Preprocessing:
Tag Stripping: Removing XML noise (<defs>, metadata).
Canonical ViewBox: Normalization to 512x512.
Complexity Filter: Removing paths with > 20 segments.(where a segment is a single draw command like `L` or `C`).

Implementation Details
A key technical optimization was 'Dynamic Padding', which reduced training time by 40% by avoiding unnecessary
padding tokens.
Key Points:

Stratified Split: Test set ( 10000 samples) balanced across categories (Nature, UI, Arrows).
Dynamic Padding:
Problem: Static padding to global max (512, that are the the maximum limit that any single sequence can
have in our dataset) wastes compute on empty tokens.
Solution: Pad only to batch max length.(If the batch has 50, 80, 120 token SVGs, only padding up to 120)
Why it matters: Attention is O(N²). Smaller N exponentially faster.
Result: ~40% Training Speedup.
Input Schema:
{"input": "<svg>...", "output": "A red arrow pointing..."}

→

Experimental Setup

Models:
Baselines:
1. Raster Models: BLIP-2, Florence-2.
2. Text-Only(Zero Shot): Qwen2 / Gemma / Llama-3.

Our Models:
1.SPE + LLM: SPE + Qwen2/SPE + Gemma
2.Text-Only(LoRA) : Qwen2 / Gemma / Llama-3 (No SPE)

Metrics:
CLIPScore: Measures how well the caption matches the image content (cosine similarity in CLIP embedding space).
BLEU/ROUGE: Measure how similar the generated caption is to the ground truth (word overlap).
Composite Score: CLIPScore/10 + BLEU + METEOR + ROUGE

Quantitative Results

Text-Only: Qwen2-7B (LoRA) high raw CLIPScore (32.30)
but hallucinations, Best Composite Score: 3.95.
SPE + LLM: SPE + Qwen2:
Best Composite Score: 4.18.
Superior Fluency: BLEU-1 (0.42) vs Baseline (0.24).
Insight: Structured embeddings act as a regularizer,
ensuring valid application of language.

Our results were revealing. While the Text-Only (LoRA) scored high on raw feature matching, it often hallucinated.
The SPE + Qwen2 model achieved the best overall Composite Score and significantly higher fluency. The structured
embedding helps the model 'ground' its language in reality

Qualitative Analysis

Case Study 1: Graduate Emoji (Icon #38):
SPE+Qwen2: "Stylized human figure with orange
shape" (Correct)
Baseline: "Hot cross bun" ✗ (Hallucination)

✓

Case Study 2: Cross/X Icon (Icon #12):
SPE+Qwen2: "Geometric figure representing an 'X'"
(Precise)
- Baseline: Generic description (Misses semantic
meaning)

✓

However, limitations exist. A prime example is 'OCR Blindness'. If you feed it a vector drawing of a STOP sign,
the model sees a red octagon. It describes the geometry perfectly but fails to read the word 'STOP' because the
text is rendered as path curves, not characters. It sees the shape, but misses the semantic symbol.

Ablation Studies
Our ablation studies showed that Z-Score Normalization is critical; without aligning the statistical distributions of
the visual encoder and the LLM, the model fails to learn. We also found that a LoRA rank of 16 provides the
perfect balance of efficiency and expressivity.

Z-Score Normalization: Critical for training stability.
Problem: SPE: embeddings have different statistics
(μ, σ) than what the LLM expects.
Solution: Normalize: z = (x - μ) / σ Aligns to
mean 0, var 1.
Without it: Training diverges (gradient explosion).

≈

≈

→

Future Work: Hierarchical Encoding

Hierarchical Encoding:
Current: SVG linearized as flat sequence.
Future: GNNs or Tree-Transformers to preserve DOM
structure.
Hybrid Dual-Stream:
Vector stream (SPE) + Raster stream (ViT) for
textures/gradients.
Late fusion via Cross-Attention.
Broader Vision:
Vector-Aware Editing: "Make the circle red" SVG
modification.
Bidirectional: Text-to-SVG generation using captioned
data.

→

Conclusions

Key Points:
Thesis Statement: LLMs can learn to see structure directly, without rasterization.
Main Achievement: Composite Score 4.18.
Outperformed Text-Only(LoRA) (+0.23) and Raster Baselines (+0.84) in quality.
Efficiency: Validated LoRA for multimodal adaptation (<1% params).
Philosophy: Moving from "Pixels as Opaque Strings" to "Graphics as Symbolic Code".

Conclusions

Thank
You

