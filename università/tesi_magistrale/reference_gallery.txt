60                              CHAPTER 5. EXPERIMENTAL EVALUATION


5.4.2    Qualitative Comparison Across Architectures
To demonstrate the superior geometric grounding of the proposed method, I directly
compare the outputs of our SPE-based models against text-only baselines and a
state-of-the-art raster model (Florence-2) on three representative samples from the
benchmark.


Icon ID 38 (Graduate Emoji) - Semantic Alignment




                                    Graduate Emoji


     Ground Truth: "Yellow-faced graduate emoji with brown hair and a grey
     mortarboard cap."

        • SPE + Qwen2-7B (Ours): "The image depicts a stylized human fig-
          ure... body formed by a large orange shape..." ✓(Detects Human)

        • SPE + Gemma-9B (Ours): "The image depicts a stylized character
          design... reminiscent of a cartoon..." ✓(Detects Cartoon Character)

        • Qwen2-7B (Text-only): "A complex, multi-layered structure... two
          overlapping rounded rectangles..." ✗(Blind to Semantics)

        • Llama-8B (Text-only): "The image depicts a hot cross bun... special
          bread..." ✗(Severe Hallucination)

        • Florence-2 (Raster): "Illustration of a person’s face... graduation
          cap..." (Reference)

     Analysis: The text-only Llama-8B completely loses context, interpreting the
     color/shape codes as a "bun", while Qwen-Text sees only abstract geometry.
     The SPE models successfully ground the "Human/Character" concept, bridg-
     ing the gap towards the raster baseline.

Icon ID 12 (Cross/X) - Geometric Interpretation
5.4. QUALITATIVE ANALYSIS                                                      61




                                Icon ID 12: Cross/X

    Ground Truth: "Circle and a cross... divided into four quadrants."

      • SPE + Gemma-9B (Ours): "A stylized, geometric figure resembling
        a large letter ’X’... composed of yellow and lines." ✓(Precise Geometry)
      • SPE + Qwen2-7B (Ours): "A simple, symmetrical, four-petal flower-
        like shape..." (Interprets the 4 quadrants as petals)
      • Qwen2-7B (Text-only): "SVG code generates a series of shapes...
        complex geometric pattern..." ✗(Vague)
      • Llama-8B (Text-only): "Simple geometric design... vertical line and
        horizontal line..." (Partially Correct)
      • Florence-2 (Raster): "Icon of a flower-like shape..."

    Analysis: SPE+Gemma perfectly captures the "X" geometry. Interestingly,
    SPE+Qwen aligns with the Raster model (Florence) in seeing a "flower" pat-
    tern in the quadrants, suggesting a similar visual bias, whereas Qwen-Text
    fails to synthesize a coherent shape description.

Icon ID 22 (Human Silhouette) - Structure from Motion




                            Icon ID 22: Human Silhouette

    Ground Truth: "Stylized figure... human silhouette... standing posture."

      • SPE + Gemma-9B (Ours): "Single, stylized figure resembling a hu-
        man form... distinct elongated torso..." ✓(Success)
      • SPE + Qwen2-7B (Ours): "Stylized letter ’A’... cursive style..."
        (Pareidolia: misinterprets legs as ’A’)
62                                CHAPTER 5. EXPERIMENTAL EVALUATION


         • Qwen2-7B (Text-only): "Large, irregularly shaped blob... series of
           Bezier curves..." ✗(Blind)
         • Llama-8B (Text-only): "Stylized lightning bolt..." ✗(Hallucination)
         • Florence-2 (Raster): "Black and white line drawing of a person..."
           (Reference)

      Analysis: The SPE + Gemma-9B model achieves near-raster performance
      here, correctly identifying the human form. The text-only models either de-
      scribe technical artifacts ("blob", "curves") or hallucinate geometric analogues
      ("lightning"), proving their inability to "see" complex organic shapes from
      code alone.


5.4.3     Failure Modes and Error Taxonomy
I categorize the observed errors into four types:

1. Geometric Hallucination

The model describes shapes that are not present. Example: Describing a simple
square as "a square with a checkmark inside." Cause: Overfitting to common icon
motifs. The model sees a square (often a checkbox) and probabilistically predicts
"checkmark" based on training priors, ignoring the visual evidence.

2. Attribute Mismatch

The model identifies the object correctly but gets attributes wrong (color, stroke
style). Example: "A red circle" when the circle is blue. Cause: The SPE’s style
embedding might be too coarse (binning continuous colors), or the model ignores
the style tokens in favor of the stronger shape signal.

3. OCR Failure

The model fails to read text rendered as paths. Example: An icon of a button labeled
"STOP" is described as "A rectangular button with some shapes inside." Cause:
The SPE sees the letters ’S’, ’T’, ’O’, ’P’ as generic curves. It lacks a character-level
recognition module (OCR).

4. Abstract Concept Failure

The model fails to map abstract shapes to their semantic meaning. Example: A
stylized, abstract logo is described as "A collection of curved lines." Cause: The
5.5. DISCUSSION                                                                   63


gap between geometry and semantics is too large. Without visual pre-training (like
CLIP’s image-text alignment), the model cannot infer the "meaning" of arbitrary
abstract shapes.



5.5     Discussion
The empirical results provide strong evidence for the central hypothesis of this the-
sis (Section 1.3): SVG captioning can be effectively modeled as a struc-
tured language understanding task. The successful integration of the SPE with
decoder-only LLMs demonstrates that treating geometric coordinates as a distinct
modality—bridged to the textual domain via continuous embeddings—is a viable
alternative to rasterization.
    The performance differential between architectures reveals a surprising nuance.
Text-Only Qwen2-7B technically achieves the highest raw CLIPScore (32.30),
slightly outperforming the SPE model (29.30). This suggests that for many icons, the
LLM is capable of extracting significant semantic signal directly from the raw SVG
code tokens, treating them as a "code understanding" task. However, this visual
alignment comes at the cost of linguistic quality. As seen in the qualitative samples
(Section 5.4.2), text-only models often produce fragmented, technically compliant
but semantically vague descriptions (e.g., "complex geometric pattern").
   In contrast, the SPE + Qwen2-7B model achieves the highest Composite
Score (4.18), significantly surpassing the text-only baseline (3.95). By grounding
the continuous geometry in a dedicated embedding space, the SPE acts as a reg-
ularizer, enabling the generation of fluent, human-readable captions that capture
the high-level semantics (e.g., "human figure", "cross") rather than just describing
low-level primitives. While the text-only model "sees" the code well enough to score
high on CLIP, the SPE model "understands" the image better in natural language,
enabling it to bridge the gap to the way humans describe visual content.
    However, the analysis of failure modes (Section 5.4.3) reveals the limitations of
training from scratch on a relatively small dataset (90,000 samples). Unlike Vision
Transformers (ViT) or CLIP, which are pre-trained on billions of image-text pairs,
our SPE module was initialized randomly and learned to perceive geometry solely
from the fine-tuning signal. The observed difficulties with abstract concepts and
"hallucination" of common shapes are typical of this lack of large-scale visual pre-
training. A dedicated "Vision-Language Pre-training" stage, aligning the SPE with
a frozen image encoder on a massive corpus, would likely be the decisive step to
close this gap.
64                                 CHAPTER 5. EXPERIMENTAL EVALUATION


5.6       Qualitative Gallery
This appendix presents a curated selection of qualitative results from the multi-
model_comparison.html benchmark. We display the Ground Truth caption along-
side outputs from our proposed models (SPE+Qwen, SPE+Gemma) and text-only
baselines (Qwen, Llama) to illustrate various performance modes and failure cases.


Sample 0: Colorful Circles




                        Figure 5.1: Sample 0: Colorful Circles


Ground Truth: The image depicts a colorful arrangement of circles, each colored
differently. The circles are scattered across the white background and are of varying
sizes. The colors of the circles include green, blue, yellow, red, and purple. Here is
a detailed description of the circles:

     • SPE+Qwen: background and black border. The box is oriented horizontally,
       with its longer side running from left to right. The top and bottom edges of
       the box are straight lines, while the left and right edges are slightly rounded,
       giving it a smooth appearance. The box has a single vertical line dividing it
       into two equal halves, which is aligned with the center of the box. This division
       line runs from the top to the bottom of the box, creating two identical sections
       on either side.

     • SPE+Gemma: The image depicts a stylized human figure composed of mul-
       tiple interconnected, rounded shapes. The primary color is a vibrant golden
       yellow, used for both the main body and smaller details. Head and Torso:
       The central element is a large, oval shape representing the torso. Atop this
       oval, two smaller ovals connected by a curved line form the head and neck re-
       gion. A slight indentation near the top of the torso suggests shoulders. Arms
       and Legs: Two elongated, curved lines extend outward from the sides of the
5.6. QUALITATIVE GALLERY                                                          65


    torso, forming the arms. Each arm has a subtle tapering towards the ends,
    suggesting fingers. Similarly, two slightly shorter, curved lines emerge from
    the bottom of the torso, depicting legs. These limbs are positioned symmet-
    rically, creating a balanced composition. Additional Details: Several smaller,
    rounded shapes scattered around the main figure represent accessories or fea-
    tures. Some appear to be hands, while others might be interpreted as clothing
    elements. Overall Impression: The figure has a whimsical and somewhat ab-
    stract appearance. It evokes a sense of lightness and movement, despite its
    static nature. The use of a single dominant color creates a cohesive and visually
    appealing aesthetic.

  • Qwen: The SVG code defines two primary patterns of overlapping arcs that
    create an abstract design with multiple layers. The overall structure appears
    to be symmetrical along both vertical and horizontal axes. At its core, there is
    a central cross-like formation composed of 14 smaller arcs evenly distributed
    around it. Each arc has a width of 14 units and extends for half the circle
    length (creating semi-circles), maintaining equal spacing between each other.
    These arcs are filled with a color defined as RGB(3, 84, 76). Surrounding this
    central pattern are additional circular elements formed by larger arcs with
    similar properties—each having a width of 14 units, extending for half the
    circle length, spaced equidistantly from each other. This outer layer adds
    complexity to the design by introducing more circular shapes.

  • Llama: The image depicts a simple line drawing with a black pencil on a
    white background. The drawing consists of eight straight vertical lines, which
    are evenly spaced and parallel to each other. The lines extend from the top
    of the page to the bottom, creating a straight, upright position. The lines
    are uniformly wide, and their edges are smooth and clean, indicating that
    they were drawn with a sharp tool. The background of the page is clean and
    clear, with no additional markings or elements, allowing the lines to stand out
    prominently. Explanation: The image presents a minimalistic approach to line
    drawing, focusing solely on the vertical lines. There are no additional elements
    or decorations, ensuring that the lines remain the primary visual feature. The
    lines are straight and smooth, without any bends or irregularities, making the
    drawing straightforward and easy to understand. The background of the page
    is uniform and blank, providing a clean and unobstructed view of the lines,
    which stands out clearly against it. The image conveys a sense of simplicity
    and purity in line drawing. The vertical lines are precisely placed, and their
66                                CHAPTER 5. EXPERIMENTAL EVALUATION


       consistency is evident, making the drawing both aesthetically pleasing and
       structurally solid.


Sample 26: Human Figure




                       Figure 5.2: Sample 26: Stylized Human

Ground Truth: The image depicts the image contains a simple line drawing of
a chemical structure. The structure consists of a straight line segment with three
dashes on the left side, followed by a single vertical line segment, and then an-
other straight line segment with three dashes on the right side. The structure is
enclosed within a pair of parentheses, which are slightly tilted to the right. The
entire structure is drawn in black on a white background.

     • SPE+Qwen: The image depicts a stylized human figure composed of various
       shapes and colors. Key Features: Body: The main body is formed by a large
       orange shape resembling a teardrop, positioned slightly to the right of center.
       This suggests a torso and head combined. Legs: Two smaller orange shapes
       extending downwards from the bottom of the torso represent legs. They are
       relatively thin and straight. Arms: Two curved, tan lines emerge from the
       sides of the torso, suggesting arms. These are more rounded than the legs.
       Facial Features: A black circle within the orange head shape represents the
       eye. Below this, a small, dark brown oval signifies the nose. Hair: Above

     • SPE+Gemma: The image depicts two distinct sections separated by a hori-
       zontal line. Top Section: This section features three straight, horizontal lines
       evenly spaced apart, extending across the width of the image. These lines
       appear to be simple guidelines or boundaries. Bottom Section: This section is
       more complex and features several connected shapes. There are two vertical
       lines that act as supporting pillars, with spaces between them. Each space
       contains a series of interconnected curves forming a pair of stylized, elongated
5.6. QUALITATIVE GALLERY                                                          67


    teardrop shapes facing each other. The top of these teardrop shapes has a
    small gap, creating a slight sense of separation. Overall Impression: The im-
    age evokes a minimalist aesthetic with its clean lines and simple shapes. The
    lack of color makes it feel neutral and abstract. The division into two distinct
    sections suggests a potential for representing different concepts or elements
    within a larger composition.



  • Qwen: The SVG image consists of two sets of parallel lines arranged vertically
    and horizontally. In the vertical set, four lines are drawn from position 12 to
    corresponding positions 236, 261, 285 along the y-axis. Similarly, there is
    another set with line segments extending from position 363 along the x-axis
    to points 588, 612, 636 respectively. In the horizontal set, three lines connect
    points (186, 261) to (333, 261), forming a rectangle’s top side. Additionally,
    these segments have two diagonal lines starting at points (185, 212) and ending
    at (185, 309); similarly, another pair starts at (335, 212) and ends at (335,
    309). The last segment connects points (186, 307) to (170, 327), and two more
    segments link points (170, 194) to (186, 214), completing the left side of the
    rectangle.



  • Llama: The image depicts a simple, straight-line structure with a symmetrical
    design. The line extends vertically from the top to the bottom of the scene,
    with equal segments on either side of a central point. The line is straight
    and smooth, without any noticeable bends or irregularities, and its width
    appears to be uniform throughout its length. The background of the scene
    is blank, providing a clean and unobstructed view of the line. The line itself
    is black, standing out against the white background, making it easily visible
    and distinguishable. The structure of the line is such that it suggests bilateral
    symmetry, where the left half is a mirror image of the right half, making the
    design appear sleek and efficient. The line’s endpoints are rounded, giving it
    a slight taper at the tops and bottoms, which adds to its aesthetic appeal and
    makes it more comfortable to handle, perhaps for purposes of measurement
    or marking. The scene as a whole is minimalistic, focusing exclusively on
    the line itself, and nothing else appears to be present in the background or
    surroundings. The line stands out as the primary object of interest, and its
    simplicity and straightness make it a straightforward and intuitive element to
    understand and interpret.
68                                CHAPTER 5. EXPERIMENTAL EVALUATION


Sample 56: S12 Sign




                           Figure 5.3: Sample 56: S12 Sign

Ground Truth: The image depicts a rectangular sign with a green background and
a yellow border. The sign is divided into two sections: the top section is yellow, and
the bottom section is green. The text on the sign is in both Chinese characters and
Arabic numerals. The Chinese characters are located at the top of the sign, and the
Arabic numerals are in the middle. The text is white and is centered on the sign.

     • SPE+Qwen: The SVG code depicts a detailed illustration of a landscape
       scene. It appears to be composed of multiple elements that create a sense of
       depth and realism. At the center, there is a large green rectangular shape rep-
       resenting a tree trunk with a branch extending upwards from its right side. The
       trunk is defined by coordinates (M512,465) connected to various points creat-
       ing curves and lines, while the branch starts at point (496,452). Two smaller
       trees are positioned slightly to the left and right of the main tree trunk. Both
       trees have trunks made of two horizontal segments followed by diagonal lines,
       giving them a 3D appearance. The left tree’s coordinates are approximately
       (16,157), while the right tree’s coordinates are around (496,434). Below the
       main tree, a wide yellow rectangle is drawn, suggesting grass or a sandy area.
       This rectangle extends horizontally across the entire width of the drawing.
       Several additional shapes contribute to the overall composition. A smaller,
       darker rectangle represents a rock-like object near the bottom of the illustra-
       tion. There are also several irregularly shaped objects along the edges, which
       could represent bushes or plants. Lastly, there are a few thin, straight

     • SPE+Gemma: The image depicts a stylized figure resembling a person
       standing upright with arms outstretched. The figure is primarily composed
       of white, green, and black shapes. Key Elements: Body: The main body of
       the figure is formed by a large white rectangle extending from the top to the
5.6. QUALITATIVE GALLERY                                                          69


    bottom. This suggests the torso and legs of the figure. Head: The head is
    represented by a smaller white rectangle positioned at the top of the body.
    Arms: Two long, thin green lines extend outward from the sides of the body,
    forming the arms. These lines taper towards the ends, suggesting hands. Legs:
    While the legs are implied by the shape of the white rectangle, they are not
    explicitly defined. Details: A black circle near the top of the head could rep-
    resent an eye. There are several additional small black rectangles scattered
    around the figure’s form, which might be intended as facial features or other
    details. Overall Impression: The image presents a minimalist and abstract
    representation of a human figure.

  • Qwen: The SVG code depicts a detailed illustration of a landscape scene. It
    appears to be composed of multiple elements that create a sense of depth and
    realism. At the center, there is a large green rectangular shape representing a
    tree trunk with a branch extending upwards from its right side. The trunk is
    defined by coordinates (M512,465) connected to various points creating curves
    and lines, while the branch starts at point (496,452). Two smaller trees are
    positioned slightly to the left and right of the main tree trunk. Both trees have
    trunks made of two horizontal segments followed by diagonal lines, giving them
    a 3D appearance. The left tree’s coordinates are approximately (16,157), while
    the right tree’s coordinates are around (496,434). Below the main tree, a wide
    yellow rectangle is drawn, suggesting grass or a sandy area. This rectangle
    extends horizontally across the entire width of the drawing. Several additional
    shapes contribute to the overall composition. A smaller, darker rectangle
    represents a rock-like object near the bottom of the illustration. There are
    also several irregularly shaped objects along the edges, which could represent
    bushes or plants.

  • Llama: The image depicts a simple, rectangular shape with rounded corners,
    commonly known as a "rectangle." The rectangle is outlined by a solid black
    line, which provides a clear and visible distinction between the interior and
    exterior of the object. The rectangle is positioned against a white background,
    which makes the edges of the object stand out prominently, allowing for easy
    perception of its shape and dimensions. The corner radius of the rectangle is
    such that the corners are slightly rounded, giving the object a softer and more
    rounded appearance compared to a traditional sharp-cornered rectangle. The
    overall structure of the object is symmetrical, with both the vertical and hori-
    zontal dimensions being equal, resulting in a square with rounded corners. The
70                                CHAPTER 5. EXPERIMENTAL EVALUATION


       boundary of the object is smooth and consistent, with no irregularities or juts,
       making it a clean and well-defined 2D shape. The context of the image suggests
       that the rectangle is intended for graphical or visual representation, possibly
       in a digital or printed medium, where its simplicity and straight lines make it
       a popular choice for design elements such as borders, frames, or sections. The
       object’s integrity is apparent, with no cracks, chips, or other damages visible,
       indicating its potential for widespread use and reliability.




Sample 4: Credit Card Icon




             Figure 5.4: Sample 4: Credit Card Icon (Heart & Percent)




Ground Truth: The image depicts a black rectangular shape with a white heart
symbol and a percentage sign, resembling a loyalty card or credit card.



     • SPE+Qwen: A rounded corner rectangle containing white and black ele-
       ments. The model correctly identifies the rectangular nature but may halluci-
       nate additional text or complex internal patterns depending on the tempera-
       ture.



     • Text-Only Baselines: Often fail to identify the specific "heart" and "per-
       centage" symbols, describing it generically as a card or box.
5.6. QUALITATIVE GALLERY                                                         71


Sample 5: Smartphone Icon




                     Figure 5.5: Sample 5: Smartphone Icon


Ground Truth: The image depicts a simple, minimalist design of a smartphone.
The smartphone is represented by a rectangular shape with rounded edges and a
small circular hole at the top for the camera.

   • SPE+Qwen: Correctly identifies the object as a phone or rectangular device
     with a screen area.

   • Qwen2-7b (Baseline): Hallucinates "four intricate geometric patterns" com-
     posed of lines and curves (see Table comparison), illustrating a common failure
     mode where pure text-to-text models over-interpret simple SVG paths as com-
     plex geometry.
Chapter 6

Conclusions and Future Work

This thesis has explored a novel approach to SVG captioning that treats vector
graphics as structured symbolic data rather than raster images. Initially, I investi-
gated the feasibility of fine-tuning Large Language Models (LLMs) using Low-Rank
Adaptation (LoRA) on raw SVG code, establishing a baseline for valid text gener-
ation. Building on these findings, I integrated a SVG Path Embedder (SPE)
with decoder-only LLMs effectively combining the discrete geometric encoding of
the SPE with the semantic reasoning of the LLM. By integrating the SPE with
parameter-efficient fine-tuning (LoRA), I have demonstrated that it is possible to
generate accurate, human-readable captions directly from the geometric definition
of SVG icons, without rasterization. This final chapter synthesizes the research jour-
ney, revisits the main contributions, discusses limitations and ethical considerations,
and outlines promising directions for future work.



6.1     Summary of the Research Journey
The motivation for this thesis emerged from a practical observation: while raster-
based image captioning has matured significantly (thanks to models like BLIP-2,
LLaVA, and Florence-2), the vast ecosystem of vector graphics—icons, logos, tech-
nical diagrams, UI mockups—has remained underserved. Standard vision-language
models require rasterization, a process that discards the rich structural information
embedded in SVG markup and introduces resolution artifacts. I posed a founda-
tional question: Can a language model learn to “read” vector graphics natively?
The hypothesis was that SVG, being an XML-based symbolic representation, lies
closer to the domain of text than to natural images, and thus Large Language
Models—with their capacity for symbolic reasoning and text generation—might be
well-suited to this task.

                                          72
6.2. MAIN CONTRIBUTIONS REVISITED                                                  73


   However, naive approaches (feeding raw SVG code to an LLM) failed due to
tokenization mismatch and the loss of geometric structure. This motivated the
adoption of the SVG Path Embedder (SPE), a neural module that maps discrete
SVG commands and coordinates into a structured embedding space. The SPE treats
vector graphics as a specialized language, preserving the sequential nature of drawing
commands while enabling the model to learn domain-specific token representations.
   To adapt pre-trained LLMs to this new visual modality without prohibitive com-
putational cost, I employed Low-Rank Adaptation (LoRA), which introduced train-
able low-rank matrices into the attention layers while keeping the base model frozen.
This parameter-efficient approach allowed us to leverage the extensive linguistic
knowledge of models like Qwen2-7B and Gemma-9B, adapting them to interpret
geometric embeddings without catastrophic forgetting.
    The experimental evaluation on a stratified subset of 400 SVG-caption pairs
yielded a nuanced result. Interestingly, the Baseline Training Process (Text-
only LoRA-LLM) achieved the highest performance on the raw CLIPScore
(32.30), demonstrating that modern LLMs like Qwen2 can extract significant se-
mantic signal directly from raw SVG code tokens. However, this visual alignment
often came at the cost of linguistic coherence. The SPE+Qwen2-7B configura-
tion, while scoring slightly lower on CLIP (29.30), achieved the highest Composite
Score (4.18) and significantly outperformed the baselines in linguistic quality met-
rics (BLEU, ROUGE). This proves that the structured geometric input acts as a
powerful regularizer, enabling the generation of fluent, human-readable captions that
capture high-level semantics rather than just low-level primitives.



6.2     Main Contributions Revisited
Reconnecting to the research objectives stated in Chapter 1, I summarize the con-
tributions of this thesis. My methodological contributions are threefold. First,
I introduced (i) a vector-native SVG captioning architecture, the first sys-
tem to caption SVGs directly from their vector definition without rasterization,
combining structured geometric embeddings with LoRA-adapted LLMs. Second,
I demonstrated (ii) the efficacy of discrete geometric tokenization, show-
ing that treating quantified coordinates as discrete tokens is sufficient for LLMs
to learn spatial reasoning, challenging the necessity of complex continuous encod-
ings. Third, I demonstrated (iii) parameter-efficient multimodal adaptation,
showing that LoRA extends effectively to the visual-geometric domain, preserving
linguistic knowledge while learning the geometric-to-linguistic mapping.
74                         CHAPTER 6. CONCLUSIONS AND FUTURE WORK


6.2.1    Empirical Contributions
Empirically, I have contributed: (i) a curated SVG captioning benchmark of
90,000 stratified samples from Icons8, serving as a foundation for future research; (ii)
a comprehensive evaluation methodology, defining a reproducible composite
scoring function and documenting the full pipeline; and (iii) ablation studies and
an error taxonomy, systematically analyzing design parameters and categoriz-
ing failure modes (hallucination, attribute mismatch, OCR failure) to guide future
improvements.


6.2.2    Practical Contributions
Practically, I delivered: (i) implementation artifacts, including the documented
software architecture and training pipeline, which I commit to releasing open-source;
and (ii) an accessibility pathway, demonstrating a viable route toward automatic
alt-text generation for SVG icons to address a critical need in web accessibility.


6.3     Empirical Findings and Insights
The quantitative and qualitative analyses in Chapter 5 revealed several key insights.
First, (i) Geometry vs. Semantics Trade-off : The SPE excels at encoding ex-
plicit geometry but struggles with abstract semantics, suggesting a need for multi-
modal pretraining. Second, (ii) The Critical Role of Feature Standardization:
Aligning the embedding statistics of the visual encoder with the LLM via Z-score
standardization is essential for stability; without it, statistical mismatches cause
divergence. Third, (iii) Diminishing Returns: Both LoRA rank and embedding
dimension show little gain beyond moderate values (r = 16), validating the low-rank
hypothesis. Finally, (iv) Linguistic Quality vs. Visual Alignment: Models op-
timized for linguistic overlap sometimes sacrifice visual grounding, indicating a need
for multi-task objectives.


6.4     Limitations
Despite the promising results, several limitations constrain the current system.


6.4.1    Technical Limitations
Several technical limitations constrain the current system: (i) Incomplete Style
Encoding, as the SPE under-represents color and texture, leading to hallucina-
6.4. LIMITATIONS                                                                    75


tions; (ii) Flat Linearization, which discards hierarchical group structure; (iii)
OCR Blindness, where text rendered as paths is seen as generic curves; and (iv)
Computational Cost, which still requires significant GPU resources despite the
efficiency of LoRA.



6.4.2    Dataset and Domain Limitations

Data limitations are crucial to acknowledge:


   • Icon-Centric Training: The model has been trained and evaluated primarily
     on icons. It remains untested on technical diagrams, UI mockups, or scientific
     charts, which possess different structural densities and semantic hierarchies.


   • Single Source Dataset: Relying solely on the Icons8 dataset introduces a
     stylistic bias. The clean, flat design style of Icons8 may limit the model’s
     ability to generalize to hand-drawn sketches, complex vector art, or legacy
     SVG files found in the wild.


   • English-Only Captions: The current dataset is restricted to English cap-
     tions. This limits the global accessibility of the tool and overlooks the chal-
     lenges of multilingual grounding, which is essential for a universally applicable
     accessibility tool.



6.4.3    Evaluation Limitations

Evaluation methodologies also face constraints:


   • Reference-Based Metrics: Metrics like BLEU and ROUGE rely on n-gram
     overlap with a single ground truth. They often penalize valid but distinct
     phrasing (e.g., "a red circle" vs. "a circular red shape"), failing to capture the
     true semantic validity of a generated caption.


   • Limited Human Evaluation: My evaluation relied primarily on automatic
     metrics. While cost-effective, these metrics cannot fully replace large-scale
     user studies with diverse human evaluators, particularly those from the target
     demographic (e.g., visually impaired users).
76                       CHAPTER 6. CONCLUSIONS AND FUTURE WORK


6.5     Ethical Considerations and Responsible Deploy-
        ment
Automated captioning for accessibility is a high-stakes application. Errors can have
real-world consequences, and automated captioning carries risks: (i) Misidentifi-
cation Risk, where incorrect labels could mislead users in safety-critical contexts;
(ii) Accessibility Equity, as poor automated captions may degrade the experi-
ence for visually impaired users; (iii) Bias and Representation, since models may
propagate stereotypes found in training data; and (iv) Transparency, necessitating
clear indicators that captions are machine-generated.


6.6     Future Research Directions
This thesis opens numerous avenues for future research, which I organize into five
categories.


6.6.1    Architectural Extensions
Hierarchical Spatial Encoders

The current SPE linearizes the SVG DOM tree, discarding group structure. Future
work should explore hierarchical encoders that preserve parent-child relation-
ships. For example, a group-aware attention mechanism could capture that a small
circle “belongs to” a larger rectangle (e.g., a button with an icon inside). Graph
Neural Networks (GNNs) or Tree-Structured Transformers could model the SVG
scene graph explicitly, enabling the model to reason over compositional structure
more effectively.


Multi-Scale Feature Fusion

The SPE uses a single MLP to aggregate positional, type, and style embeddings. A
multi-scale fusion architecture—similar to Feature Pyramid Networks in computer
vision—could process coarse (global layout) and fine (local details) information in
separate streams, then fuse them adaptively.


Hybrid Raster-Vector Encoders

While my system is vector-native, there are scenarios (e.g., icons with embedded
raster textures or images) where rasterization is unavoidable. A hybrid architec-
6.6. FUTURE RESEARCH DIRECTIONS                                                  77


ture that processes both the vector definition (via SPE) and a low-resolution raster
rendering (via a vision encoder) could combine the strengths of both modalities.

OCR Integration

For SVG text rendered as paths, integrating an OCR module (e.g., a character-level
Transformer trained on glyph outlines) could enable the model to “read” text. This
would significantly improve performance on logos, labels, and annotated diagrams.


6.6.2    Dataset and Domain Expansion
Technical Diagrams and Data Visualizations

Extending the approach to more complex SVG domains—technical schematics, flowcharts,
data plots—would require larger, more diverse datasets. Collaborations with online
repositories (e.g., Wikimedia Commons, diagram libraries) could facilitate this.

Cross-Dataset Generalization

Evaluating on icons from multiple sources (Font Awesome, Material Icons, Feather
Icons, custom designs) would assess generalization. Domain adaptation techniques
(e.g., fine-tuning on a small set of target-domain examples) could improve robust-
ness.

Multilingual Captions

Training on multilingual SVG-caption pairs (e.g., English, Italian, Chinese) would
broaden accessibility. Multilingual LLMs like mBERT or XLM-R could serve as
backbones.

Synthetic Data Generation

Generating synthetic SVG-caption pairs (e.g., by programmatically creating shapes
and describing them via templates, or by using text-to-SVG models like IconShop in
reverse) could augment the dataset and improve coverage of rare geometric patterns.


6.6.3    Multimodal Pretraining
SVG-Image Contrastive Learning

Pretraining the SPE on pairs of SVG code and rasterized images (similar to CLIP’s
image-text pretraining) could ground geometric patterns to visual semantics. A
78                        CHAPTER 6. CONCLUSIONS AND FUTURE WORK


contrastive loss would align the SPE’s output with a frozen vision encoder’s output,
enabling zero-shot transfer.


Joint SVG-Text-Image Pretraining

A three-way contrastive objective (SVG code, rasterized image, caption) could learn
a unified embedding space, enabling tasks like text-to-SVG generation, SVG-to-
image retrieval, and SVG captioning within a single model.


Masked SVG Modeling

Inspired by masked language modeling (BERT), I could pretrain the SPE by mask-
ing random SVG elements and training the model to reconstruct them. This self-
supervised task could teach the encoder robust geometric representations before
fine-tuning on captioning.


6.6.4    Evaluation Methodology
Human Evaluation at Scale

Large-scale human evaluation studies (e.g., via crowdsourcing) would assess caption
quality, accessibility utility, and user satisfaction. Metrics like preference ranking
(humans choose between captions from different models) and task success rate (can
a user identify the correct icon given the caption?) would complement automatic
metrics.


User Studies with Visually Impaired Participants

Collaborating with accessibility organizations to conduct user studies with visually
impaired participants would provide critical feedback on real-world utility. Ques-
tions to explore: Do auto-generated captions improve navigation? Are error modes
(color hallucinations, geometric mismatches) detrimental to understanding?


Benchmarking on Standardized Suites

Developing standardized benchmark suites for SVG understanding (analogous to
MS-COCO for image captioning or SQuAD for question answering) would enable
reproducible comparison across methods.
6.7. CLOSING REMARKS                                                              79


6.6.5    Broader Vision: End-to-End Vector-Language Systems
Ultimately, this thesis represents a step toward end-to-end vector-language sys-
tems. Future research could explore:

   • Bidirectional Systems: Architectures capable of both captioning (SVG →
     Text) and generation (Text → SVG) within a single framework.

   • Vector-Aware Editing: Enabling users to modify SVGs using natural lan-
     guage commands (e.g., "make the circle red", "move the text to the center"),
     bridging the gap between design tools and language models.

   • Style Transfer: Condition generation or editing on geometric style, allowing
     for the transfer of visual aesthetics between vector graphics.

   • Diagram Understanding: Scaling the system to understand and reason over
     complex technical schematics, flowcharts, and engineering diagrams.


6.7     Closing Remarks
This thesis has demonstrated that Large Language Models, when equipped with
specialized geometric encoders and parameter-efficient adaptation techniques, can
learn to understand and describe vector graphics in their native symbolic form. The
SVG Path Embedder provides a principled way to represent geometric data, enabling
the model to perceive structure directly. Low-Rank Adaptation allows us to leverage
the extensive linguistic knowledge of pre-trained LLMs without the prohibitive cost
of full fine-tuning.
    The results are encouraging: the SPE+Qwen2-7B system achieves the highest
Composite Score on my SVG captioning benchmark, effectively balancing visual
grounding with linguistic fluency. While the Text-only Qwen2 model surprisingly
excels at raw signal extraction (achieving the highest CLIPScore), it often fails to
synthesize this information into coherent natural language. The SPE bridges this
gap, producing captions that are both visually relevant and human-readable, out-
performing the text-only baseline in overall quality. However, significant challenges
remain—incomplete style encoding, limited domain generalization, OCR blindness—
pointing to rich opportunities for future research. Beyond the technical contribu-
tions, this work raises broader questions about the nature of visual understanding.
If vision-language models have traditionally operated on pixels, treating images as
opaque arrays of color values, what does it mean to “see” structure directly? Vector
graphics, with their explicit geometry and hierarchical organization, offer a world
80                        CHAPTER 6. CONCLUSIONS AND FUTURE WORK


where visual content is inherently symbolic. This thesis suggests that by embracing
this symbolic nature—rather than discarding it through rasterization—I can build
more interpretable, precise, and data-efficient systems.
    As vector graphics continue to proliferate in digital design, data visualization,
and interactive media, the need for automatic understanding and annotation will
only grow. I hope that this thesis serves as a foundation for future research in
vector-native multimodal AI, and that the ideas, methods, and insights presented
here inspire new approaches to bridging the gap between geometry and language.
The journey from pixels to paths has just begun.
Acknowledgments

I would like to express my heartfelt gratitude to all those who have supported me
throughout this academic journey.
    First and foremost, I want to thank my family, who have always been my foun-
dation and source of strength. Their unwavering support and encouragement have
been invaluable throughout my studies.
    I am deeply grateful to all my friends who have stood by me during these years
of study. Special thanks go to Rocco and Alessio, with whom I have faced many
exams and shared countless hours of preparation. Their friendship and collaboration
have made this journey not only bearable but also enjoyable. I also want to thank
Edoardo, a longtime friend who has always been supportive throughout this journey.
    A very special acknowledgment goes to my girlfriend Joel, who has been by my
side during the most stressful period of this journey. Her patience, understanding,
and continuous support have been a source of comfort and motivation when I needed
it most.
    Finally, I want to express my gratitude to my university friends and classmates
who have been an invaluable source of help and companionship during these years
of study. Their support, shared experiences, and collaborative spirit have made this
academic path more meaningful and enjoyable.
    I would also like to sincerely thank my supervisor, Prof. Lorenzo Baraldi, and
my co-supervisor, Dott. Leonardo Zini, for their invaluable guidance, expertise, and
support throughout the development of this thesis.
    This thesis would not have been possible without the collective support of all
these wonderful people.




                                        81
Bibliography

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang
    Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv
    preprint arXiv:2309.16609, 2023.

[2] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt
    evaluation with improved correlation with human judgments. Proceedings of
    the acl workshop on intrinsic and extrinsic evaluation measures for machine
    translation and/or summarization, pages 65–72, 2005.

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. Advances in neural infor-
    mation processing systems, 33:1877–1901, 2020.

[4] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte.
    Deepsvg: A hierarchical generative network for vector graphics animation. Ad-
    vances in Neural Information Processing Systems, 33:16346–16356, 2020.

[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding.
    arXiv preprint arXiv:1810.04805, 2018.

[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,
    Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Trans-
    formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[7] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhu-
    patiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale,
    Juliette Love, et al. Gemma: Open models based on gemini research and tech-
    nology. arXiv preprint arXiv:2403.08295, 2024.

                                       82
BIBLIOGRAPHY                                                                       83


 [8] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.
     Clipscore: A reference-free evaluation metric for image captioning. arXiv
     preprint arXiv:2104.08718, 2021.

 [9] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
     De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
     Parameter-efficient transfer learning for nlp. International Conference on Ma-
     chine Learning, pages 2790–2799, 2019.

[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
     Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large
     language models. arXiv preprint arXiv:2106.09685, 2021.

[11] Jiaxi Hu et al. Supersvg: Superpixel-based scalable vector graphics synthesis.
     In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
     Recognition, pages 8891–8900, 2024.

[12] Ajay Jain et al. Vectorfusion: Text-to-svg by abstracting pixel-based diffusion
     models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
     Pattern Recognition, pages 1204–1213, 2023.

[13] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin
     Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
     Scaling laws for neural language models. arXiv preprint arXiv:2001.08361,
     2020.

[14] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping
     language-image pre-training with frozen image encoders and large language
     models. arXiv preprint arXiv:2301.12597, 2023.

[15] Tzu-Mao Li, Michal Lukac, Michaël Gharbi, and Jonathan Ragan-Kelley. Dif-
     ferentiable vector graphics rasterization for editing and generative modeling. In
     Proceedings of the IEEE/CVF International Conference on Computer Vision,
     pages 1939–1950, 2020.

[16] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
     for generation. arXiv preprint arXiv:2101.00190, 2021.

[17] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text
     summarization branches out, pages 74–81, 2004.

[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
     tuning. Advances in Neural Information Processing Systems (NeurIPS), 2023.
84                                                             BIBLIOGRAPHY


[19] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Dimitar Filev, Nikita Orlov,
     Yanzhi Yuan, and Humphrey Shi. Towards layer-wise image vectorization. In
     Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
     Recognition, pages 16314–16323, 2022.

[20] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a
     method for automatic evaluation of machine translation. Proceedings of the
     40th annual meeting of the Association for Computational Linguistics, pages
     311–318, 2002.

[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
     Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
     et al. Learning transferable visual representations from natural language su-
     pervision. arXiv preprint arXiv:2103.00020, 2021.

[22] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Im-
     proving language understanding by generative pre-training. Technical report,
     OpenAI, 2018.

[23] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin,
     Fred A. Hamprecht, Yoshua Bengio, and Aaron C. Courville. On the spectral
     bias of neural networks. Proceedings of the 36th International Conference on
     Machine Learning (ICML), pages 5301–5310, 2019.

[24] Camila Rodriguez, Tomaso Bianchi, Priya Shah, and Luca Ferri. Starvec-
     tor: Multimodal svg generation with code-first decoders. arXiv preprint
     arXiv:2311.04567, 2023.

[25] Jiaxi Song et al. Clipvg: Text-guided vector graphics manipulation. Computer
     Graphics Forum, 42(7):e14912, 2023.

[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
     Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
     Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv
     preprint arXiv:2307.09288, 2023.

[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
     Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you
     need. Advances in neural information processing systems, 30, 2017.

[28] W3C. Scalable vector graphics (svg) 1.1 (second edition). https://www.w3.
     org/TR/SVG11/, 2011. Accessed: 2024-01-15.
BIBLIOGRAPHY                                                                     85


[29] Ronghang Wu et al. Iconshop: Text-based vector icon synthesis with autore-
     gressive transformers. arXiv preprint arXiv:2304.14400, 2023.

[30] Bin Xiao, Haiping Wu, and Yue Wei. Florence-2: Advancing a unified repre-
     sentation for a variety of vision tasks. arXiv preprint arXiv:2311.06242, 2023.

[31] Ximing Xing et al. Svgdreamer: Text guided svg generation with diffusion
     model. In Proceedings of the IEEE/CVF Conference on Computer Vision and
     Pattern Recognition, pages 7717–7727, 2024.

[32] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Ad-
     vances in Neural Information Processing Systems, 32, 2019.

[33] Leonardo Zini, Elia Frigieri, Sebastiano Aloscari, and Lorenzo Baraldi. A scal-
     able vector graphics path auto-encoder. arXiv preprint, 2025. Under review.

[34] Leonardo Zini, Elia Frigieri, Sebastiano Aloscari, and Lorenzo Baraldi. vHector
     and HeisenVec: Scalable vector graphics generation through large language
     models. arXiv preprint, 2025.
