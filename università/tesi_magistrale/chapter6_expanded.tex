\chapter{Conclusions and Future Work}
\label{chap:conclusioni}

This thesis has explored a novel approach to SVG captioning that treats vector graphics as structured symbolic data rather than raster images. Initially, I investigated the feasibility of fine-tuning Large Language Models (LLMs) using Low-Rank Adaptation (LoRA) on raw SVG code, establishing a baseline for valid text generation. Building on these findings, I integrated a \textbf{SVG Path Embedder (SPE)} with decoder-only LLMs effectively combining the discrete geometric encoding of the SPE with the semantic reasoning of the LLM. By integrating the SPE with parameter-efficient fine-tuning (LoRA), I have demonstrated that it is possible to generate accurate, human-readable captions directly from the geometric definition of SVG icons, without rasterization. This final chapter synthesizes the research journey, revisits the main contributions, discusses limitations and ethical considerations, and outlines promising directions for future work.

\section{Summary of the Research Journey}
\label{sec:research-journey}

The motivation for this thesis emerged from a practical observation: while raster-based image captioning has matured significantly (thanks to models like BLIP-2, LLaVA, and Florence-2), the vast ecosystem of vector graphics--icons, logos, technical diagrams, UI mockups--has remained underserved. Standard vision-language models require rasterization, a process that discards the rich structural information embedded in SVG markup and introduces resolution artifacts. I posed a foundational question: \textit{Can a language model learn to ``read'' vector graphics natively?} The hypothesis was that SVG, being an XML-based symbolic representation, lies closer to the domain of text than to natural images, and thus Large Language Models--with their capacity for symbolic reasoning and text generation--might be well-suited to this task.

However, naive approaches (feeding raw SVG code to an LLM) failed due to tokenization mismatch and the loss of geometric structure. This motivated the adoption of the \textbf{SVG Path Embedder (SPE)}, a neural module that maps discrete SVG commands and coordinates into a structured embedding space. The SPE treats vector graphics as a specialized language, preserving the sequential nature of drawing commands while enabling the model to learn domain-specific token representations.

To adapt pre-trained LLMs to this new visual modality without prohibitive computational cost, I employed Low-Rank Adaptation (LoRA), which introduced trainable low-rank matrices into the attention layers while keeping the base model frozen. This parameter-efficient approach allowed us to leverage the extensive linguistic knowledge of models like Qwen2-7B and Gemma-9B, adapting them to interpret geometric embeddings without catastrophic forgetting.

The experimental evaluation on a stratified subset of 400 SVG-caption pairs yielded a nuanced result. Interestingly, the \textbf{Baseline Training Process (Text-only LoRA-LLM)} achieved the highest performance on the \textbf{raw CLIPScore (32.30)}, demonstrating that modern LLMs like Qwen2 can extract significant semantic signal directly from raw SVG code tokens. However, this visual alignment often came at the cost of linguistic coherence. The \textbf{SPE+Qwen2-7B} configuration, while scoring slightly lower on CLIP (29.30), achieved the highest \textbf{Composite Score (4.18)} and significantly outperformed the baselines in linguistic quality metrics (BLEU, ROUGE). This proves that the structured geometric input acts as a powerful regularizer, enabling the generation of fluent, human-readable captions that capture high-level semantics rather than just low-level primitives.

\section{Main Contributions Revisited}
\label{sec:contributions-revisited}

Reconnecting to the research objectives stated in Chapter~\ref{chap:introduzione}, I summarize the contributions of this thesis. My methodological contributions are threefold. First, I introduced (i) \textbf{a vector-native SVG captioning architecture}, the first system to caption SVGs directly from their vector definition without rasterization, combining structured geometric embeddings with LoRA-adapted LLMs. Second, I demonstrated (ii) \textbf{the efficacy of discrete geometric tokenization}, showing that treating quantified coordinates as discrete tokens is sufficient for LLMs to learn spatial reasoning, challenging the necessity of complex continuous encodings. Third, I demonstrated (iii) \textbf{parameter-efficient multimodal adaptation}, showing that LoRA extends effectively to the visual-geometric domain, preserving linguistic knowledge while learning the geometric-to-linguistic mapping.

\subsection{Empirical Contributions}

Empirically, I have contributed: (i) \textbf{a curated SVG captioning benchmark} of 90,000 stratified samples from Icons8, serving as a foundation for future research; (ii) \textbf{a comprehensive evaluation methodology}, defining a reproducible composite scoring function and documenting the full pipeline; and (iii) \textbf{ablation studies and an error taxonomy}, systematically analyzing design parameters and categorizing failure modes (hallucination, attribute mismatch, OCR failure) to guide future improvements.

\subsection{Practical Contributions}

Practically, I delivered: (i) \textbf{implementation artifacts}, including the documented software architecture and training pipeline, which I commit to releasing open-source; and (ii) \textbf{an accessibility pathway}, demonstrating a viable route toward automatic alt-text generation for SVG icons to address a critical need in web accessibility.

\section{Empirical Findings and Insights}
\label{sec:empirical-insights}

The quantitative and qualitative analyses in Chapter~\ref{chap:valutazione} revealed several key insights. First, (i) \textbf{Geometry vs. Semantics Trade-off}: The SPE excels at encoding explicit geometry but struggles with abstract semantics, suggesting a need for multimodal pretraining. Second, (ii) \textbf{The Critical Role of Feature Standardization}: Aligning the embedding statistics of the visual encoder with the LLM via Z-score standardization is essential for stability; without it, statistical mismatches cause divergence. Third, (iii) \textbf{Diminishing Returns}: Both LoRA rank and embedding dimension show little gain beyond moderate values ($r=16$), validating the low-rank hypothesis. Finally, (iv) \textbf{Linguistic Quality vs. Visual Alignment}: Models optimized for linguistic overlap sometimes sacrifice visual grounding, indicating a need for multi-task objectives.

\section{Limitations}
\label{sec:limitations-conclusions}

Despite the promising results, several limitations constrain the current system.

\subsection{Technical Limitations}

Several technical limitations constrain the current system: (i) \textbf{Incomplete Style Encoding}, as the SPE under-represents color and texture, leading to hallucinations; (ii) \textbf{Flat Linearization}, which discards hierarchical group structure; (iii) \textbf{OCR Blindness}, where text rendered as paths is seen as generic curves; and (iv) \textbf{Computational Cost}, which still requires significant GPU resources despite the efficiency of LoRA.

\subsection{Dataset and Domain Limitations}

Data limitations are crucial to acknowledge:
\begin{itemize}
    \item \textbf{Icon-Centric Training}: The model has been trained and evaluated primarily on icons. It remains untested on technical diagrams, UI mockups, or scientific charts, which possess different structural densities and semantic hierarchies.
    \item \textbf{Single Source Dataset}: Relying solely on the Icons8 dataset introduces a stylistic bias. The clean, flat design style of Icons8 may limit the model's ability to generalize to hand-drawn sketches, complex vector art, or legacy SVG files found in the wild.
    \item \textbf{English-Only Captions}: The current dataset is restricted to English captions. This limits the global accessibility of the tool and overlooks the challenges of multilingual grounding, which is essential for a universally applicable accessibility tool.
\end{itemize}

\subsection{Evaluation Limitations}

Evaluation methodologies also face constraints:
\begin{itemize}
    \item \textbf{Reference-Based Metrics}: Metrics like BLEU and ROUGE rely on n-gram overlap with a single ground truth. They often penalize valid but distinct phrasing (\textit{e.g.}, "a red circle" vs. "a circular red shape"), failing to capture the true semantic validity of a generated caption.
    \item \textbf{Limited Human Evaluation}: My evaluation relied primarily on automatic metrics. While cost-effective, these metrics cannot fully replace large-scale user studies with diverse human evaluators, particularly those from the target demographic (\textit{e.g.}, visually impaired users).
\end{itemize}

\section{Ethical Considerations and Responsible Deployment}
\label{sec:ethical-considerations-conclusions}

Automated captioning for accessibility is a high-stakes application. Errors can have real-world consequences, and automated captioning carries risks: (i) \textbf{Misidentification Risk}, where incorrect labels could mislead users in safety-critical contexts; (ii) \textbf{Accessibility Equity}, as poor automated captions may degrade the experience for visually impaired users; (iii) \textbf{Bias and Representation}, since models may propagate stereotypes found in training data; and (iv) \textbf{Transparency}, necessitating clear indicators that captions are machine-generated.

\section{Future Research Directions}
\label{sec:future-work}

This thesis opens numerous avenues for future research, which I organize into five categories.

\subsection{Architectural Extensions}

\subsubsection{Hierarchical Spatial Encoders}

The current SPE linearizes the SVG DOM tree, discarding group structure. Future work should explore \textbf{hierarchical encoders} that preserve parent-child relationships. For example, a group-aware attention mechanism could capture that a small circle ``belongs to'' a larger rectangle (\textit{e.g.}, a button with an icon inside). Graph Neural Networks (GNNs) or Tree-Structured Transformers could model the SVG scene graph explicitly, enabling the model to reason over compositional structure more effectively.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/hierarchical_encoder.png}
    \caption{\textbf{Hierarchical vs. Flat Encoding}. A conceptual comparison between processing the SVG DOM as a hierarchical tree (left), allowing for structure-aware encoding, versus the current flat serialization approach (right).}
    \label{fig:hierarchical_encoder}
\end{figure}

\subsubsection{Multi-Scale Feature Fusion}

The SPE uses a single MLP to aggregate positional, type, and style embeddings. A multi-scale fusion architecture--similar to Feature Pyramid Networks in computer vision--could process coarse (global layout) and fine (local details) information in separate streams, then fuse them adaptively.

\subsubsection{Hybrid Raster-Vector Encoders}

While my system is vector-native, there are scenarios (\textit{e.g.}, icons with embedded raster textures or images) where rasterization is unavoidable. A hybrid architecture that processes both the vector definition (via SPE) and a low-resolution raster rendering (via a vision encoder) could combine the strengths of both modalities.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/hybrid_architecture.png}
    \caption{\textbf{Proposed Hybrid Architecture}. A dual-stream network that processes raster images (via CNN/ViT) and vector code (via SPE) in parallel, fusing the representations to leverage both visual texture and geometric precision.}
    \label{fig:hybrid_architecture}
\end{figure}

\subsubsection{OCR Integration}

For SVG text rendered as paths, integrating an OCR module (\textit{e.g.}, a character-level Transformer trained on glyph outlines) could enable the model to ``read'' text. This would significantly improve performance on logos, labels, and annotated diagrams.

\subsection{Dataset and Domain Expansion}

\subsubsection{Technical Diagrams and Data Visualizations}

Extending the approach to more complex SVG domains--technical schematics, flowcharts, data plots--would require larger, more diverse datasets. Collaborations with online repositories (\textit{e.g.}, Wikimedia Commons, diagram libraries) could facilitate this.

\subsubsection{Cross-Dataset Generalization}

Evaluating on icons from multiple sources (Font Awesome, Material Icons, Feather Icons, custom designs) would assess generalization. Domain adaptation techniques (\textit{e.g.}, fine-tuning on a small set of target-domain examples) could improve robustness.

\subsubsection{Multilingual Captions}

Training on multilingual SVG-caption pairs (\textit{e.g.}, English, Italian, Chinese) would broaden accessibility. Multilingual LLMs like mBERT or XLM-R could serve as backbones.

\subsubsection{Synthetic Data Generation}

Generating synthetic SVG-caption pairs (\textit{e.g.}, by programmatically creating shapes and describing them via templates, or by using text-to-SVG models like IconShop in reverse) could augment the dataset and improve coverage of rare geometric patterns.

\subsection{Multimodal Pretraining}

\subsubsection{SVG-Image Contrastive Learning}
Pretraining the SPE on pairs of SVG code and rasterized images (similar to CLIP's image-text pretraining) could ground geometric patterns to visual semantics. A contrastive loss would align the SPE's output with a frozen vision encoder's output, enabling zero-shot transfer.

\subsubsection{Joint SVG-Text-Image Pretraining}
A three-way contrastive objective (SVG code, rasterized image, caption) could learn a unified embedding space, enabling tasks like text-to-SVG generation, SVG-to-image retrieval, and SVG captioning within a single model.

\subsubsection{Masked SVG Modeling}
Inspired by masked language modeling (BERT), I could pretrain the SPE by masking random SVG elements and training the model to reconstruct them. This self-supervised task could teach the encoder robust geometric representations before fine-tuning on captioning.

\subsection{Evaluation Methodology}
\label{sec:eval-methodology-future}

\subsubsection{Human Evaluation at Scale}
Large-scale human evaluation studies (\textit{e.g.}, via crowdsourcing) would assess caption quality, accessibility utility, and user satisfaction. Metrics like preference ranking (humans choose between captions from different models) and task success rate (can a user identify the correct icon given the caption?) would complement automatic metrics.

\subsubsection{User Studies with Visually Impaired Participants}
Collaborating with accessibility organizations to conduct user studies with visually impaired participants would provide critical feedback on real-world utility. Questions to explore: Do auto-generated captions improve navigation? Are error modes (color hallucinations, geometric mismatches) detrimental to understanding?

\subsubsection{Benchmarking on Standardized Suites}
Developing standardized benchmark suites for SVG understanding (analogous to MS-COCO for image captioning or SQuAD for question answering) would enable reproducible comparison across methods.

\subsection{Broader Vision: End-to-End Vector-Language Systems}
\label{sec:broader-vision}

Ultimately, this thesis represents a step toward \textbf{end-to-end vector-language systems}. Future research could explore:
\begin{itemize}
    \item \textbf{Bidirectional Systems}: Architectures capable of both captioning (SVG $\rightarrow$ Text) and generation (Text $\rightarrow$ SVG) within a single framework.
    \item \textbf{Vector-Aware Editing}: Enabling users to modify SVGs using natural language commands (\textit{e.g.}, "make the circle red", "move the text to the center"), bridging the gap between design tools and language models.
    \item \textbf{Style Transfer}: Condition generation or editing on geometric style, allowing for the transfer of visual aesthetics between vector graphics.
    \item \textbf{Diagram Understanding}: Scaling the system to understand and reason over complex technical schematics, flowcharts, and engineering diagrams.
\end{itemize}

\section{Closing Remarks}
\label{sec:closing-remarks}

This thesis has demonstrated that Large Language Models, when equipped with specialized geometric encoders and parameter-efficient adaptation techniques, can learn to understand and describe vector graphics in their native symbolic form. The SVG Path Embedder provides a principled way to represent geometric data, enabling the model to perceive structure directly. Low-Rank Adaptation allows us to leverage the extensive linguistic knowledge of pre-trained LLMs without the prohibitive cost of full fine-tuning.

The results are encouraging: the SPE+Qwen2-7B system achieves the highest \textbf{Composite Score} on my SVG captioning benchmark, effectively balancing visual grounding with linguistic fluency. While the \textbf{Text-only Qwen2} model surprisingly excels at raw signal extraction (achieving the highest CLIPScore), it often fails to synthesize this information into coherent natural language. The SPE bridges this gap, producing captions that are both visually relevant and human-readable, outperforming the text-only baseline in overall quality. However, significant challenges remain---incomplete style encoding, limited domain generalization, OCR blindness---pointing to rich opportunities for future research. Beyond the technical contributions, this work raises broader questions about the nature of visual understanding. If vision-language models have traditionally operated on pixels, treating images as opaque arrays of color values, what does it mean to ``see'' structure directly? Vector graphics, with their explicit geometry and hierarchical organization, offer a world where visual content is inherently symbolic. This thesis suggests that by embracing this symbolic nature---rather than discarding it through rasterization---I can build more interpretable, precise, and data-efficient systems.

As vector graphics continue to proliferate in digital design, data visualization, and interactive media, the need for automatic understanding and annotation will only grow. I hope that this thesis serves as a foundation for future research in vector-native multimodal AI, and that the ideas, methods, and insights presented here inspire new approaches to bridging the gap between geometry and language. The journey from pixels to paths has just begun.
