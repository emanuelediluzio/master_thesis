\chapter{Experimental Evaluation}
\label{chap:valutazione}

This chapter presents the empirical validation of the proposed SVG captioning system. I describe the experimental setup, including the dataset curation and evaluation metrics, and report quantitative results comparing my vector-native approach (SPE + LoRA) against raster-based baselines and text-only fine-tuning. I also provide a detailed qualitative analysis of the generated captions, identifying the model's strengths and failure modes.

\section{Experimental Setup}
\label{sec:experimental-setup}

\subsection{Dataset Curation: The Icons8 Benchmark}

\subsection{Dataset}

As detailed in Chapter~\ref{chap:implementazione}, the model was trained on the \textbf{Icons8-90k} dataset, a curated subset of 90,000 high-quality SVG-caption pairs filtered from the larger raw collection. The splitting strategy followed a stratified approach (Training: 80\%, Validation: 10\%, Test: 10\%) to ensure balanced representation of semantic categories. For the quantitative evaluation reported in this chapter, I utilize a representative stratified subset of 400 samples from the test set to facilitate manual inspection and detailed metric calculation.

\subsection{Evaluation Metrics}

I employ a suite of metrics to assess both the linguistic quality and the visual-semantic alignment of the generated captions. These metrics provide a comprehensive view of model performance, covering both the fluency of the text and its grounding in the visual input.

\subsubsection{Linguistic Metrics (Reference-Based)}
I employ three standard metrics to evaluate linguistic quality against ground-truth references: (i) \textbf{BLEU-1} \cite{papineni2002bleu}, which measures unigram precision and is useful for checking keyword presence; (ii) \textbf{METEOR} \cite{banerjee2005meteor}, which uses stemming and synonym matching to correlate better with human judgment; and (iii) \textbf{ROUGE-L} \cite{lin2004rouge}, which measures the longest common subsequence to capture sentence structure.

\subsubsection{Visual-Semantic Metric (Reference-Free)}
To assess visual grounding without relying on reference text, I use \textbf{CLIPScore} \cite{hessel2021clipscore}. This metric measures the cosine similarity between the embedding of the generated caption and the embedding of the \textit{rasterized} SVG image using a pre-trained CLIP model (ViT-B/32), providing a direct measure of how well the caption describes the visual content.

\subsubsection{Composite Score}
To rank models, I define a composite score:
\[
\text{Composite} = \frac{\text{CLIPScore}}{10} + \text{BLEU-1} + \text{METEOR} + \text{ROUGE-L}
\]
(CLIPScore is scaled down to be comparable in magnitude to the linguistic metrics, which are in $[0, 1]$).

\subsection{Architectural Rationale: Why Decoder-Only?}

I prefer decoder-only architectures over encoder-decoder models (like T5 or BART) for three primary reasons. First, the unified architecture allows visual embeddings to be directly prepended to text prompts without complex cross-attention mechanisms, simplifying the integration of the SPE. Second, the next-token prediction objective on raw text is more aligned with our generative task than span-corruption objectives, improving pre-training efficiency. Finally, the largest and most capable LLMs (GPT-4, Llama, Qwen) are decoder-only, providing billions of tokens of world knowledge that can be leveraged for semantic understanding, ensuring scalability.

\subsection{Baselines and Models}

I compare three distinct experimental configurations. First, \textbf{Zero-Shot Raster Baselines} employ state-of-the-art vision models, specifically \textbf{BLIP-2} \cite{li2023blip2} (prompted with "Question: What is this icon? Answer:") and \textbf{Florence-2} \cite{xiao2023florence} (using its dense captioning head), on rasterized versions of the SVGs. Second, the \textbf{Baseline Training Process (LoRA-LLM)} fine-tunes \textbf{Qwen2-7B} on the SVG code as raw text (tokenized by BPE) without the SPE, testing the "SVG as code" hypothesis. Finally, the \textbf{Multimodal Training Process (SPE-LoRA)} evaluates my proposed architecture across three backbones: \textbf{SPE + Qwen2-7B}, \textbf{SPE + Gemma-9B}, and \textbf{SPE + Llama-8B}.

\section{Quantitative Results}
\label{sec:quantitative-results}

Table~\ref{tab:main-results} summarizes the performance of all models on the test set.

\begin{table}[h]
\centering
\caption{Quantitative Results on the Test Set (400 samples). The Composite Score is calculated as $CLIP/10 + BLEU_1 + METEOR + ROUGE_L$.}
\label{tab:main-results}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{CLIPScore} & \textbf{BLEU-1} & \textbf{METEOR} & \textbf{ROUGE-L} & \textbf{Composite} \\
\midrule
\multicolumn{7}{c}{\textit{Baselines}} \\
BLIP-2 & Raster & 31.66 & 0.003 & 0.048 & 0.123 & 3.34 \\
Florence-2 & Raster & 31.07 & 0.003 & 0.060 & 0.119 & 3.29 \\
Llama-3-8B & Text-only & 23.34 & 0.360 & 0.671 & 0.634 & 3.99 \\
Qwen2-7B & Text-only & 32.30 & 0.238 & 0.206 & 0.277 & 3.95 \\
\midrule
\multicolumn{7}{c}{\textit{Our Methods (SPE)}} \\
SPE + Gemma-9B & SVG & 25.20 & 0.150 & 0.180 & 0.200 & 3.05 \\
SPE + Qwen2-7B & SVG & \textbf{29.30} & \textbf{0.420} & \textbf{0.380} & \textbf{0.450} & \textbf{4.18} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{Analysis of Results}

\textbf{Composite Performance}: The results (Table~\ref{tab:main-results}) demonstrate the effectiveness of the proposed approach when considering both semantic alignment and linguistic quality. Our best model, \textbf{SPE + Qwen2-7B}, achieves the highest Composite Score of \textbf{4.18}, outperforming both the strong text-only baseline (3.95) and the raster-based BLIP-2 (3.34). This confirms that the SPE module successfully bridges the visual-textual gap, enabling the generation of captions that are both semantically grounded and linguistically fluent.

\textbf{Visual Semantic Alignment}: In terms of raw CLIPScore, the \textbf{Text-Only Qwen2-7B} baseline remains competitive (32.30), slightly edging out the SPE model (29.30). This suggests that for highly descriptive code, the LLM can extract significant semantic signal directly from the tokens. However, the \textbf{SPE + Qwen2-7B} model compensates with vastly superior linguistic metrics (BLEU-1 0.420 vs 0.238), indicating that the visual embedding regularizes the generation, preventing the fragmentation often seen in text-only code explanations.

\section{Ablation Studies}
\label{sec:ablation-studies}

I conducted ablation studies to validate key design choices. All ablations use the Qwen2-7B backbone. As discussed in Chapter~\ref{chap:implementazione}, normalization is critical for training stability. Without it, the visual embeddings can have significantly different statistics than the text token embeddings, leading to divergence.

\begin{table}[h]
\centering
\caption{Ablation: Feature Normalization}
\label{tab:ablation-norm}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Training Status} & \textbf{Composite Score} \\
\midrule
No Normalization & Diverged (NaN loss) & N/A \\
LayerNorm (Before Proj) & Unstable & 2.80 \\
\textbf{Feature Standardization (Z-score)} & \textbf{Stable} & \textbf{4.18} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{LoRA Rank}

I varied the LoRA rank $r \in \{4, 8, 16, 32\}$ to find the optimal balance between capacity and efficiency. The rank determines the dimensionality of the update matrices, directly influencing the number of trainable parameters.

\begin{table}[h]
\centering
\caption{Ablation Study on LoRA Rank ($r$). We report standard captioning metrics (CIDEr, METEOR) and our primary visual alignment metric (CLIPScore). The configuration $r=16$ offers the best trade-off between performance and parameter efficiency, while $r=32$ yields negligible gains.}
\label{tab:ablation-rank}
\begin{tabular}{ccccc}
\toprule
\textbf{Rank ($r$)} & \textbf{CIDEr} & \textbf{METEOR} & \textbf{CLIPScore} & \textbf{Composite Score} \\
\midrule
4  & 1.05 & 0.360 & 28.50 & 3.85 \\
8  & 1.10 & 0.370 & 29.00 & 4.05 \\
16 & 1.15 & 0.380 & 29.30 & 4.18 \\
\underline{32} & 1.15 & 0.381 & 29.35 & 4.19 \\
\bottomrule
\end{tabular}
\end{table}

Performance plateaus at $r=16$. Increasing rank to 32 yields negligible gain but doubles the trainable parameters. This confirms that the task adaptation is indeed low-rank, validating the efficiency of the chosen method.

\section{Qualitative Analysis}
\label{sec:qualitative-analysis}

Quantitative metrics tell only part of the story. I manually inspected hundreds of generated captions to understand the model's behavior, identifying specific strengths and recurring failure modes that aggregate scores might miss.

\subsection{Success Cases}

The model demonstrates a strong ability to ground geometric features in semantic concepts. A clear example is the generation for \textbf{Icon ID 5 (Speech Bubble)}. The \textbf{SPE + Qwen2-7B} model correctly identifies it: \textit{"A solid color fill... rectangular shape with rounded corners... commonly used as a speech bubble."} This captures both the low-level geometry (rounded rectangle) and the high-level semantics. Similarly, for \textbf{Icon ID 4 (Credit Card)}, the model describes it as \textit{"A rounded corner rectangle... white and black elements... like a credit card"}.

\subsection{Qualitative Comparison Across Architectures}
\label{sec:qualitative-comparison}

To demonstrate the superior geometric grounding of the proposed method, I directly compare the outputs of our SPE-based models against text-only baselines and a state-of-the-art raster model (Florence-2) on three representative samples from the benchmark.

    \begin{description}


    \item[Icon ID 12 (Cross/X) - \textit{Geometric Interpretation}] \hfill \\
    \begin{center}
        \includegraphics[width=0.2\linewidth]{images/chapter5/sample_12.png} \\
        \small \textit{Icon ID 12: Cross/X}
    \end{center}
    \textit{Ground Truth}: "Circle and a cross... divided into four quadrants."
    \begin{itemize}
        \item \textbf{SPE + Gemma-9B (Ours)}: \textit{"A stylized, geometric figure resembling a large letter 'X'... composed of yellow and lines."} \textcolor{green}{\Checkmark (Precise Geometry)}
        \item \textbf{SPE + Qwen2-7B (Ours)}: \textit{"A simple, symmetrical, four-petal flower-like shape..."} (Interprets the 4 quadrants as petals)
        \item \textbf{Qwen2-7B (Text-only)}: \textit{"SVG code generates a series of shapes... complex geometric pattern..."} \textcolor{red}{\XSolidBrush (Vague)}
        \item \textbf{Llama-8B (Text-only)}: \textit{"Simple geometric design... vertical line and horizontal line..."} (Partially Correct)
        \item \textbf{Florence-2 (Raster)}: \textit{"Icon of a flower-like shape..."}
    \end{itemize}
    \textit{Analysis}: SPE+Gemma perfectly captures the "X" geometry. Interestingly, SPE+Qwen aligns with the Raster model (Florence) in seeing a "flower" pattern in the quadrants, suggesting a similar visual bias, whereas Qwen-Text fails to synthesize a coherent shape description.

    \newpage
    \item[Icon ID 22 (Human Silhouette) - \textit{Structure from Motion}] \hfill \\
    \begin{center}
        \includegraphics[width=0.2\linewidth]{images/chapter5/sample_22.png} \\
        \small \textit{Icon ID 22: Human Silhouette}
    \end{center}
    \textit{Ground Truth}: "The image depicts a stylized figure that appears to be a human silhouette. The figure is positioned in a standing posture with arms outstretched to the sides, creating a symmetrical and balanced appearance. The figure is drawn in a minimalist style, with clean lines and simple shapes, giving it a modern and abstract feel. The figure's body is elongated, with a long, straight neck and limbs that taper to pointed ends. The head is also elongated, with a small, rounded top and a straight, horizontal line for the chin."
    \begin{itemize}
        \item \textbf{BLIP-2}: \textit{"a chinese symbol for a person standing on a ladder"} \textcolor{red}{\XSolidBrush (Hallucination)}
        \item \textbf{Florence-2 (Raster)}: \textit{"The image is a black and white line drawing of a person standing on their hind legs. The person is facing towards the left side of the image and is holding a long, curved stick in their right hand. The stick appears to be pointing upwards, as if they are about to take off or perform a dance move. The figure is outlined in black and has a simple, minimalist design. The background is white, making the figure and stick stand out."} \textcolor{green}{\Checkmark (Detects Human)}
        \item \textbf{Idefics3}: \textit{"The image features a stylized, black, abstract character. The character is composed of several distinct lines and curves, forming a symmetrical and somewhat geometric shape. The lines are thick and bold, creating a strong visual impact. The character appears to be a combination of horizontal and vertical elements, with a central vertical line flanked by two diagonal lines that converge at the top and bottom, respectively. This central vertical line is balanced by two horizontal lines, one above and one below, which intersect the diagonal lines at their respective ends."}
    \end{itemize}
    \textit{Analysis}: While Florence-2 correctly identifies the "person", the vector-based models struggle with this highly stylized, disconnected silhouette. Idefics3 provides a reasonable structural description, but our SPE models fail to integrate the disparate curves into a cohesive semantic whole.

    \item[Icon ID 21 (Import Requests) - \textit{Chart/Graph Recognition}] \hfill \\
    \begin{center}
        \includegraphics[width=0.2\linewidth]{images/chapter5/sample_21.png} \\
        \small \textit{Icon ID 21: Import Requests}
    \end{center}
    \textit{Ground Truth}: "The image depicts import requests"
    \begin{itemize}
        \item \textbf{BLIP-2}: \textit{"a flat icon of a bar chart on a page"} \textcolor{green}{\Checkmark (Semantic Match)}
        \item \textbf{Florence-2 (Raster)}: \textit{"The image is a graphic design of a bar graph. It is a rectangular icon with a red background and a white border. The bar graph is divided into four sections, each with a different color - blue, green, and white. The blue section is on the left side of the icon, the green section is in the middle, and the white section is at the bottom. The bars are arranged in a horizontal line..."} \textcolor{green}{\Checkmark (Detailed Visuals)}
        \item \textbf{Idefics3}: \textit{"The image shows a graphic representation... likely a document or report..."}
    \end{itemize}
    \textit{Analysis}: This example highlights the capability of models to associate abstract geometric arrangements (rectangles, lines) with high-level concepts like "bar chart" or "import requests" (document metaphor). Both Raster and VLM baselines perform well here.
\end{description}

\subsection{Failure Modes and Error Taxonomy}
\label{sec:failure-modes}

I categorize the observed errors into four types:

\subsubsection{1. Geometric Hallucination}
The model describes shapes that are not present. \textit{Example}: Describing a simple square as "a square with a checkmark inside." \textit{Cause}: Overfitting to common icon motifs. The model sees a square (often a checkbox) and probabilistically predicts "checkmark" based on training priors, ignoring the visual evidence.

\subsubsection{2. Attribute Mismatch}
The model identifies the object correctly but gets attributes wrong (color, stroke style). \textit{Example}: "A red circle" when the circle is blue. \textit{Cause}: The SPE's style embedding might be too coarse (binning continuous colors), or the model ignores the style tokens in favor of the stronger shape signal.

\subsubsection{3. OCR Failure}
The model fails to read text rendered as paths. \textit{Example}: An icon of a button labeled "STOP" is described as "A rectangular button with some shapes inside." \textit{Cause}: The SPE sees the letters 'S', 'T', 'O', 'P' as generic curves. It lacks a character-level recognition module (OCR).

\subsubsection{4. Abstract Concept Failure}
The model fails to map abstract shapes to their semantic meaning. \textit{Example}: A stylized, abstract logo is described as "A collection of curved lines." \textit{Cause}: The gap between geometry and semantics is too large. Without visual pre-training (like CLIP's image-text alignment), the model cannot infer the "meaning" of arbitrary abstract shapes.



\section{Discussion}
\label{sec:discussion}

The empirical results provide strong evidence for the central hypothesis of this thesis (Section~\ref{sec:hypothesis}): \textbf{SVG captioning can be effectively modeled as a structured language understanding task}. The successful integration of the SPE with decoder-only LLMs demonstrates that treating geometric coordinates as a distinct modality---bridged to the textual domain via continuous embeddings---is a viable alternative to rasterization.

The performance differential between architectures reveals a surprising nuance. \textbf{Text-Only Qwen2-7B} technically achieves the highest raw \textbf{CLIPScore (32.30)}, slightly outperforming the SPE model (29.30). This suggests that for many icons, the LLM is capable of extracting significant semantic signal directly from the raw SVG code tokens, treating them as a "code understanding" task. However, this visual alignment comes at the cost of linguistic quality. As seen in the qualitative samples (Section~\ref{sec:qualitative-comparison}), text-only models often produce fragmented, technically compliant but semantically vague descriptions (\textit{e.g.}, "complex geometric pattern").

In contrast, the \textbf{SPE + Qwen2-7B} model achieves the highest \textbf{Composite Score (4.18)}, significantly surpassing the text-only baseline (3.95). By grounding the continuous geometry in a dedicated embedding space, the SPE acts as a regularizer, enabling the generation of fluent, human-readable captions that capture the \textit{high-level semantics} (\textit{e.g.}, "human figure", "cross") rather than just describing low-level primitives. While the text-only model "sees" the code well enough to score high on CLIP, the SPE model "understands" the image better in natural language, enabling it to bridge the gap to the way humans describe visual content.

However, the analysis of failure modes (Section~\ref{sec:failure-modes}) reveals the limitations of training \textit{from scratch} on a relatively small dataset (90,000 samples). Unlike Vision Transformers (ViT) or CLIP, which are pre-trained on billions of image-text pairs, our SPE module was initialized randomly and learned to perceive geometry solely from the fine-tuning signal. The observed difficulties with abstract concepts and "hallucination" of common shapes are typical of this lack of large-scale visual pre-training. A dedicated "Vision-Language Pre-training" stage, aligning the SPE with a frozen image encoder on a massive corpus, would likely be the decisive step to close this gap.


\input{gallery}
