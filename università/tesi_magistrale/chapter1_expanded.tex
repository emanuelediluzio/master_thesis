\chapter{Introduction}
\label{chap:introduzione}

The interpretation of visual information by artificial intelligence has undergone a remarkable transformation over the past decade. From early Convolutional Neural Networks (CNNs) designed to classify static images, to modern Vision Transformers (ViTs) capable of understanding complex scenes, the field has consistently pushed the boundaries of what machines can ``see'' and comprehend. Yet, despite these advances, a fundamental assumption has remained largely unchallenged: that visual data consists of pixel grids--discrete arrays of color intensity values.

This pixel-centric paradigm, while successful for photographs and natural imagery, overlooks a critical dimension of modern digital content. A substantial portion of the visual material I encounter daily--icons, logos, technical diagrams, user interface elements, typographic designs--is not created from pixels at all. Instead, these graphics are defined as \textbf{Scalable Vector Graphics (SVG)}, mathematical descriptions of shapes, curves, and paths that are inherently resolution-independent and semantically structured.

Consider a simple icon: a magnifying glass representing ``search'' functionality on a website. To a human designer, this icon is not a collection of colored dots, but a composition of geometric primitives--a circle for the lens, a line for the handle, perhaps a smaller circle to suggest a reflection. The icon is encoded as a sequence of XML instructions that describe these shapes precisely:
\begin{itemize}
    \item \texttt{<circle cx="45" cy="45" r="35"/>} defines the lens;
    \item \texttt{<line x1="70" y1="70" x2="100" y2="100"/>} defines the handle.
\end{itemize}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[scale=0.05, yscale=-1, every node/.style={scale=1}]
        % The Magnifying Glass Icon (Optimized Geometry)
        \draw[line width=3pt] (45,45) circle (35);
        \draw[line width=3pt, line cap=round] (70,70) -- (100,100);
    \end{tikzpicture}
    \caption{Visual representation of the simple SVG magnifying glass defined by the XML code.}
    \label{fig:intro_magnifying_glass}
\end{figure}
\noindent This representation is compact, editable, and infinitely scalable; zooming in never introduces pixelation or blur. However, when state-of-the-art Multimodal Large Language Models (MLLMs) --- systems like GPT-4 with Vision (GPT-4V), LLaVA, or BLIP-2 --- are asked to describe such an icon, they cannot read the vector definition directly. Instead, the standard practice is to \textbf{rasterize} the SVG: render it to a fixed-resolution bitmap (\textit{e.g.}, 512$\times$512 pixels), then feed this raster image to a vision encoder. This approach has two significant drawbacks. First, it discards the rich semantic structure embedded in the SVG markup--the hierarchy of groups, the annotations in \texttt{id} and \texttt{class} attributes, the explicit geometric relationships between shapes. Second, it introduces aliasing and discretization artifacts that can obscure fine details, especially for icons with thin lines or small features.

This thesis challenges the prevailing pixel-based paradigm by posing a simple question: \textit{Can I teach a language model to "read" vector graphics natively, without converting them to raster images first?} The motivation for this question is both practical and philosophical. On the practical side, there is an urgent need for automatic captioning systems that can generate accurate textual descriptions of digital graphics. On the philosophical side, treating SVGs as a \textit{language} aligns naturally with the strengths of Large Language Models (LLMs).

\section{The Challenge of Vector Graphics Captioning}
\label{sec:challenge-vector-captioning}

To appreciate the difficulty of the task, I must first understand what makes SVG fundamentally different from raster images. An SVG file is an XML document conforming to the W3C Scalable Vector Graphics specification \cite{w3c2011svg}. Unlike a JPEG or PNG, which stores a fixed grid of pixel values, an SVG stores a \textit{hierarchical DOM tree}--a structured set of graphical elements. Each element (\textit{e.g.}, \texttt{<path>}, \texttt{<circle>}, \texttt{<rect>}) is defined by a set of attributes: geometric parameters (coordinates, radii, control points), style properties (fill color, stroke width, opacity), and possibly metadata (titles, descriptions, ARIA labels).

The flexibility of this representation is a double-edged sword. On one hand, it enables powerful features like infinite zoom, easy editing, and style inheritance. On the other hand, it introduces significant variability. The same visual icon can be encoded in countless ways: paths can be absolute or relative, clockwise or counterclockwise; shapes can be grouped or flattened; transformations (translate, rotate, scale) can be applied at any level of the hierarchy. Furthermore, the coordinate values are \textit{continuous}--floating-point numbers that specify precise locations on a two-dimensional canvas. This continuous nature poses a challenge for standard text tokenization schemes, which typically operate on discrete vocabularies.

Moreover, the ``vocabulary'' of SVG includes geometric concepts that do not map straightforwardly to the semantic categories learned by vision-language models trained on natural images. A curve defined by a cubic BÃ©zier path has no direct pixel-level analogue; it is a parametric function. Understanding that a specific sequence of path commands draws, say, a magnifying glass, requires integrating the geometric information across multiple elements and inferring the high-level semantic category from the emergent shape.

This gap between geometry and semantics is precisely where existing Multimodal Large Language Models (MLLMs) struggle. Standard vision encoders--whether ResNet-based or Vision Transformer-based--are designed to extract features from dense pixel grids. They excel at recognizing textures, edges, and spatial patterns in natural photos, but they have no mechanism to directly ingest the symbolic, structured representation of an SVG. Consequently, the common workaround is rasterization: render the SVG to a pixel image and process it as if it were a photograph. This approach, while expedient, is fundamentally mismatched to the data modality. It is analogous to asking someone to describe a written sentence by first printing it, photographing the page, and then performing OCR--technically feasible, but wasteful and error-prone.

\section{Accessibility, Search, and the Need for Automatic Captions}
\label{sec:motivation-accessibility}

The practical importance of SVG captioning extends far beyond academic curiosity. In the context of web accessibility, the World Wide Web Consortium (W3C) Web Content Accessibility Guidelines (WCAG) mandate that all non-text content be accompanied by alternative text (alt-text) that conveys the same information. For images, this is typically a short caption describing the visual content. However, many websites, design systems, and icon libraries contain thousands of SVG icons, and manually writing captions for each is a labor-intensive and often neglected task.

Consider a large design system like Google's Material Icons or Font Awesome, which provide hundreds of icons for user interface elements. Each icon needs a descriptive label not only for accessibility compliance but also for searchability. A designer looking for a ``notification bell'' icon should be able to search for that phrase and find the relevant SVG. Similarly, screen reader users navigating a web page should hear ``notification bell icon'' rather than the generic fallback ``image'' or, worse, the cryptic SVG filename ``icon\_287.svg''.

Current practice often relies on manually assigned tags or filenames, which are inconsistent and prone to error. An automatic captioning system that can generate accurate, human-readable descriptions directly from the vector definition would significantly streamline the design workflow, improve accessibility, and enable more sophisticated search and retrieval applications (\textit{e.g.}, semantic similarity search across icon libraries). Furthermore, as vector graphics are increasingly used in domains beyond traditional iconography--technical diagrams, data visualizations, UI mockups, schematic drawings--the ability to automatically annotate and describe these graphics becomes even more valuable. For instance, in educational contexts, automatically generating descriptions of geometric diagrams could aid visually impaired students. In software engineering, automatically documenting UI components from their SVG prototypes could improve code documentation and design handoff.

\section{Research Hypothesis: Treating SVG as a Structured Language}
\label{sec:hypothesis}

The central hypothesis of this thesis is that SVG captioning is best approached not as a vision problem (processing pixels), but as a \textbf{structured language understanding problem}. Since an SVG is fundamentally a symbolic representation--a sequence of XML tags and numeric coordinates--it lies closer to the domain of text than to the domain of natural images. This observation suggests that Large Language Models (LLMs), which are inherently designed to process and generate sequences of symbols, might be well-suited to this task.

However, a naive approach --- simply feeding the raw SVG XML code as a text string to an LLM --- fails in practice. The coordinate values are verbose (\textit{e.g.}, \texttt{d="M12.5,34.2..."}) and the numeric precision is lost during text tokenization. Early experiments with zero-shot prompting produced generic or hallucinatory captions.

To address this, I first investigate a \textbf{text-only fine-tuning baseline}, where the LLM is adapted to process linearized SVG code as a standard language modeling task. While this improves syntactic understanding, it remains limited in grasping complex geometries. Therefore, I propose a hybrid approach that bridges the gap between the continuous geometric domain and the discrete symbolic domain of language. Specifically, I integrate a \textbf{SVG Path Embedder (SPE)}--a neural module developed by Zini \textit{et al.} \cite{zini2025vhector}--that maps the continuous $(x, y)$ coordinates of SVG elements into a high-dimensional embedding space compatible with the LLM's input representation. This encoder does not operate in isolation; it must be integrated with a powerful language generation component that can transform the geometric features into fluent natural language, which is where Large Language Models enter the picture.

\section{The Role of Large Language Models in the Proposed Approach}
\label{sec:llms-role}

Why use Large Language Models at all for this task? The answer lies in their remarkable capacity for semantic understanding and text generation. LLMs like GPT-4, Llama, Qwen, and Gemma are pre-trained on vast corpora of text, giving them extensive world knowledge and the ability to generate fluent, contextually appropriate natural language. Crucially, these models are \textit{decoder-only} architectures, meaning they generate text autoregressively--one token at a time--conditioned on a prefix of input tokens.

In my architecture, the SPE produces a sequence of visual embeddings corresponding to the geometric elements of the SVG. These embeddings are then prepended (or interleaved) with the LLM's text token embeddings, and the model is tasked with generating a caption autoregressively. The LLM's pre-trained linguistic knowledge allows it to map the geometric features to semantic categories (\textit{e.g.}, recognizing that a circle with a line forms a ``magnifying glass''), and to generate grammatically correct, human-readable descriptions. However, off-the-shelf LLMs are not trained to interpret geometric embeddings. Their input space consists of text tokens (words, subwords, characters), not continuous vector representations of shapes. Therefore, I must \textit{adapt} the LLM to this new modality. This adaptation is achieved through fine-tuning: I train the model on a dataset of paired SVG-caption examples, teaching it to associate the geometric embeddings produced by the SPE with the corresponding textual descriptions.

\section{Parameter-Efficient Fine-Tuning with LoRA}
\label{sec:peft-intro}

Fine-tuning a modern LLM with billions of parameters is computationally expensive and risks \textit{catastrophic forgetting}--the model may overfit to the new task and lose its general linguistic capabilities. Full fine-tuning also requires storing a separate copy of the entire model for each task, which is impractical when deploying multiple specialized systems.

To mitigate these issues, I employ \textbf{Parameter-Efficient Fine-Tuning (PEFT)}, specifically \textbf{Low-Rank Adaptation (LoRA)} \cite{hu2021lora}. LoRA freezes the pre-trained weights of the LLM and introduces small, trainable low-rank matrices that modify the model's behavior. This drastically reduces the number of trainable parameters (from billions to millions) and accelerates training, while preserving the core linguistic knowledge of the base model. In my experiments, I apply LoRA to the attention projection matrices (query, key, value, and output projections) of the Transformer layers. During training, only the LoRA matrices and the SPE parameters are updated; the LLM's pre-trained weights remain frozen. This not only makes training feasible on limited hardware (\textit{e.g.}, a single or pair of GPUs), but also ensures that the adapted model retains its ability to generate fluent, grammatically correct captions.

\section{The SVG Path Embedder: Bridging Geometry and Language}
\label{sec:spe-intro}

With the LLM and the fine-tuning strategy established, I arrive at the final piece of the puzzle: the SVG Path Embedder (SPE). The SPE is a custom neural module designed to convert the continuous geometric properties of SVG elements into dense vector representations that the LLM can process. I provide a high-level preview here; the full technical details, including design alternatives and ablation studies, are developed in Chapter~\ref{chap:metodologia}.

The core challenge is encoding continuous coordinates. Unlike discrete token indices in text (which have a natural ordering and are easily represented as one-hot vectors or learned embeddings), $(x, y)$ coordinates are real-valued and do not fit naturally into the vocabulary of a language model. Naive discretization (\textit{e.g.}, rounding to the nearest integer or binning into fixed ranges) introduces quantization artifacts and limits the model's ability to perceive fine geometric details.

Instead, the SPE employs a technique inspired by the sinusoidal positional encodings used in the original Transformer architecture and by Fourier feature mappings in Neural Radiance Fields (NeRF). The core idea is to project each coordinate value through a bank of sinusoidal functions with geometrically increasing frequencies:
\[
\text{PE}(x) = \left[ \sin(\omega_1 x), \cos(\omega_1 x), \sin(\omega_2 x), \cos(\omega_2 x), \ldots, \sin(\omega_k x), \cos(\omega_k x) \right]^T
\]
where $\omega_k = 10000^{2k/d}$ for $k = 0, 1, \ldots, d/2 - 1$. This projection maps each scalar coordinate to a high-dimensional vector. The lower frequencies capture coarse, global position information (``is this shape in the top-left or bottom-right of the canvas?''), while the higher frequencies encode fine-grained details (``is this corner sharp or slightly rounded?''). By concatenating these positional encodings with learnable embeddings for element type (circle, path, rectangle, etc.) and style attributes (fill, stroke, etc.), and passing the result through a multi-layer perceptron (MLP), the SPE produces a unified embedding for each SVG element. These embeddings are then linearly projected to match the dimensionality of the LLM's input space and normalized to ensure numerical stability during training.

\section{Research Objectives}
\label{sec:research-objectives}

The primary objective of this thesis is to develop and validate a system capable of generating accurate, human-readable textual captions for SVG graphics by processing their vector definition directly, without rasterization. This broad goal is decomposed into four specific research objectives: (i) to \textbf{develop and validate a Baseline Training Process (LoRA-LLM)}, implementing a text-only fine-tuning pipeline where the SVG code is treated as a raw string to establish a strong baseline; (ii) to \textbf{develop and validate a Multimodal Training Process (SPE-LoRA)}, integrating the \textbf{SVG Path Embedder (SPE)} with decoder-only LLMs via parameter-efficient fine-tuning to enable true geometric understanding; (iii) to \textbf{curate a rigorous evaluation benchmark} consisting of a diverse, stratified test set of SVG-caption pairs; and (iv) to \textbf{conduct quantitative and qualitative analysis} of the model's performance, identifying strengths and failure modes.



\section{Summary of Methodology and Key Findings}
\label{sec:summary-methodology}

This thesis introduces a vector-native SVG captioning pipeline centered on the SVG Path Embedder (SPE), which maps geometric and structural properties of SVG elements into a continuous latent space compatible with Large Language Models. Unlike traditional raster-based approaches (BLIP-2, Florence-2), which render SVGs to pixel grids and process them with vision encoders, my method preserves the vector nature of the data and leverages the symbolic reasoning capabilities of LLMs.

The core methodology involves three stages: (1) SVG preprocessing and linearization, where the XML scene graph is traversed and path commands are extracted and normalized; (2) LoRA-based fine-tuning of the LLM, initially established as a text-only baseline to adapt the model to SVG syntax; and (3) geometric embedding via the SPE, which projects continuous coordinates through sinusoidal features to enable the full multimodal approach.

Experimentally, I evaluate the system on a dataset of 90,000 SVG-caption pairs from Icons8, with a stratified test set of 400 samples. I compare zero-shot raster baselines (BLIP-2, Florence-2), the \textbf{Baseline Training Process (LoRA-LLM)} (text-only fine-tuning on SVG code), and the \textbf{Multimodal Training Process (SPE-LoRA)} (my proposed vector-native approach). The results demonstrate that the \textbf{SPE+Qwen2-7B} configuration achieves the highest composite score (\textbf{8.89}, calculated as $\text{CLIPScore}/10 + \text{BLEU} + \text{METEOR} + \text{ROUGE}$), outperforming both zero-shot baselines and the text-only baseline in linguistic quality metrics (BLEU-1: \textbf{0.420}, METEOR: \textbf{0.380}, ROUGE-L: \textbf{0.450}), while maintaining competitive visual-semantic alignment (CLIPScore: \textbf{29.30}).

Qualitative analysis reveals that the model excels at describing geometric primitives and capturing spatial relationships in well-defined icon categories (User Interface, Arrows, Weather), but struggles with abstract shapes, color attribution (when style features are not encoded), and text recognition in path-based glyphs. These findings highlight the importance of feature normalization in the SPE pipeline and suggest directions for future work, including hierarchical encoding of SVG group structures and integration of style attributes.

\section{Thesis Organization}
\label{sec:thesis-organization}



The remainder of this manuscript is organized as follows. \textbf{Chapter 2} provides a comprehensive survey of the theoretical foundations and related work, moving from deep learning fundamentals to vision-language models and the specific technologies underpinning this thesis. \textbf{Chapter 3} presents the proposed methodology in detail, describing the design rationale for the SVG Path Embedder, the LoRA-based fine-tuning strategy, and the geometric intuition behind the architecture. \textbf{Chapter 4} documents the implementation, covering the software architecture, data loading pipeline, and training workflow. \textbf{Chapter 5} presents the experimental evaluation, reporting quantitative results for all model configurations and providing a detailed qualitative analysis of case studies and failure modes. Finally, \textbf{Chapter 6} synthesizes the findings, discusses limitations and ethical considerations, and outlines promising directions for future research. In the following chapters, I develop each of these themes in depth, building a rigorous and comprehensive account of the SVG captioning problem, my proposed solution, and the empirical evidence supporting its effectiveness.
