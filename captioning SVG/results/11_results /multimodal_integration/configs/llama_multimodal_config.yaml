# ðŸŸ  CONFIGURAZIONE LLAMA-3.1-8B MULTIMODALE

model:
  name: "llama-3.1-8b-multimodal"
  base_model: "meta-llama/Llama-3.1-8B-Instruct"
  
  # Dimensioni modello
  hidden_size: 4096
  num_attention_heads: 32
  num_hidden_layers: 32
  vocab_size: 128256
  max_position_embeddings: 131072

encoder:
  # DA DEFINIRE CON LEONARDO
  output_dim: null  # TBD
  sequence_length: null  # TBD
  model_type: null  # TBD (ResNet, ViT, etc.)

adapter:
  type: "linear_projection"
  input_dim: null  # encoder.output_dim
  output_dim: 4096  # model.hidden_size
  dropout: 0.1
  activation: "gelu"
  layer_norm: true

integration:
  method: "prepend_visual_tokens"
  visual_token_length: null  # TBD
  special_tokens:
    - "<image>"
    - "</image>"
    - "<visual>"
    - "</visual>"

training:
  strategy: "freeze_llm_train_adapter"
  learning_rate: 1e-4
  batch_size: 4
  gradient_accumulation_steps: 8
  max_epochs: 10
  warmup_steps: 100
  
  # LoRA config (se necessario)
  use_lora: false
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

inference:
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  do_sample: true
  pad_token_id: 128001  # eos_token_id

paths:
  # Paths relativi a multimodal_integration/
  encoder_weights: "encoder_weights/image_encoder.pth"
  projection_weights: "encoder_weights/projection_layer.pth"
  train_embeddings: "embeddings/train_embeddings.pkl"
  test_embeddings: "embeddings/test_embeddings.pkl"
  
  # Output paths
  adapter_save_path: "experiments/llama_multimodal/adapter.pth"
  results_dir: "experiments/llama_multimodal/results"
  logs_dir: "experiments/llama_multimodal/logs"

evaluation:
  metrics:
    - "bleu"
    - "rouge"
    - "meteor"
    - "cider"
    - "clip_score"
  
  test_set_size: 400
  batch_size: 8

hardware:
  device: "auto"
  mixed_precision: true
  gradient_checkpointing: true
