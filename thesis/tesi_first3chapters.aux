\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{5}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Sommario}{6}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduzione}{{1}{7}{Introduction}{chapter.1}{}}
\citation{w3c2011svg}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Challenge of Vector Graphics Captioning}{8}{section.1.1}\protected@file@percent }
\newlabel{sec:challenge-vector-captioning}{{1.1}{8}{The Challenge of Vector Graphics Captioning}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Accessibility, Search, and the Need for Automatic Captions}{9}{section.1.2}\protected@file@percent }
\newlabel{sec:motivation-accessibility}{{1.2}{9}{Accessibility, Search, and the Need for Automatic Captions}{section.1.2}{}}
\citation{zini2025vhector}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Research Hypothesis: Treating SVG as a Structured Language}{10}{section.1.3}\protected@file@percent }
\newlabel{sec:hypothesis}{{1.3}{10}{Research Hypothesis: Treating SVG as a Structured Language}{section.1.3}{}}
\citation{hu2021lora}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}The Role of Large Language Models in the Proposed Approach}{11}{section.1.4}\protected@file@percent }
\newlabel{sec:llms-role}{{1.4}{11}{The Role of Large Language Models in the Proposed Approach}{section.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Parameter-Efficient Fine-Tuning with LoRA}{11}{section.1.5}\protected@file@percent }
\newlabel{sec:peft-intro}{{1.5}{11}{Parameter-Efficient Fine-Tuning with LoRA}{section.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}The SVG Path Embedder: Bridging Geometry and Language}{12}{section.1.6}\protected@file@percent }
\newlabel{sec:spe-intro}{{1.6}{12}{The SVG Path Embedder: Bridging Geometry and Language}{section.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Research Objectives}{13}{section.1.7}\protected@file@percent }
\newlabel{sec:research-objectives}{{1.7}{13}{Research Objectives}{section.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Summary of Methodology and Key Findings}{13}{section.1.8}\protected@file@percent }
\newlabel{sec:summary-methodology}{{1.8}{13}{Summary of Methodology and Key Findings}{section.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Thesis Organization}{14}{section.1.9}\protected@file@percent }
\newlabel{sec:thesis-organization}{{1.9}{14}{Thesis Organization}{section.1.9}{}}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:stato-arte}{{2}{15}{State of the Art}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Foundations: Transformers and Self-Attention}{15}{section.2.1}\protected@file@percent }
\newlabel{sec:dl-foundations}{{2.1}{15}{Foundations: Transformers and Self-Attention}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Transformer Architecture}{15}{subsection.2.1.1}\protected@file@percent }
\citation{zhang2019root}
\citation{devlin2018bert}
\citation{radford2018improving,brown2020language}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}SVG and the Tokenization Challenge}{16}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Large Language Models: Evolution to Decoder-Only Architectures}{16}{section.2.2}\protected@file@percent }
\newlabel{sec:llms-evolution}{{2.2}{16}{Large Language Models: Evolution to Decoder-Only Architectures}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}From BERT to GPT: Encoder vs. Decoder}{16}{subsection.2.2.1}\protected@file@percent }
\citation{touvron2023llama}
\citation{qwen2023}
\citation{gemma2024}
\citation{kaplan2020scaling}
\citation{dosovitskiy2020image}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Modern Decoder-Only LLMs}{17}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Why Decoder-Only for SVG Captioning?}{17}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Vision-Language Models and Their Limitations}{17}{section.2.3}\protected@file@percent }
\newlabel{sec:vision-language}{{2.3}{17}{Vision-Language Models and Their Limitations}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Vision Transformers (ViT)}{17}{subsection.2.3.1}\protected@file@percent }
\citation{radford2021learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Vision Transformer (ViT) Architecture. The image is split into patches, linearly projected, and processed by a standard Transformer encoder. (Source: Dosovitskiy et al., 2020)}}{18}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:vit-architecture}{{2.1}{18}{The Vision Transformer (ViT) Architecture. The image is split into patches, linearly projected, and processed by a standard Transformer encoder. (Source: Dosovitskiy et al., 2020)}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}CLIP: Contrastive Language-Image Pre-training}{18}{subsection.2.3.2}\protected@file@percent }
\citation{li2023blip2}
\citation{liu2023llava}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Architecture of CLIP (Contrastive Language-Image Pre-training). The model learns to align raster images and text in a shared embedding space, enabling zero-shot transfer to downstream tasks.}}{19}{figure.caption.5}\protected@file@percent }
\newlabel{fig:clip-architecture}{{2.2}{19}{Architecture of CLIP (Contrastive Language-Image Pre-training). The model learns to align raster images and text in a shared embedding space, enabling zero-shot transfer to downstream tasks}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Generative Captioning: BLIP-2 and LLaVA}{19}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}The Vector Graphics Gap}{19}{subsection.2.3.4}\protected@file@percent }
\citation{vaswani2017attention}
\citation{rahaman2019spectral}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Positional and Coordinate Encodings}{20}{section.2.4}\protected@file@percent }
\newlabel{sec:positional-encodings}{{2.4}{20}{Positional and Coordinate Encodings}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Sinusoidal Encodings: A Fourier Perspective}{20}{subsection.2.4.1}\protected@file@percent }
\newlabel{eq:sinusoidal-encoding}{{2.1}{20}{Sinusoidal Encodings: A Fourier Perspective}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Application to SVG Coordinates}{21}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Parameter-Efficient Fine-Tuning (PEFT)}{21}{section.2.5}\protected@file@percent }
\newlabel{sec:peft-methods}{{2.5}{21}{Parameter-Efficient Fine-Tuning (PEFT)}{section.2.5}{}}
\citation{houlsby2019parameter}
\citation{li2021prefix}
\citation{hu2021lora}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Visualization of 2D Sinusoidal Positional Encodings. High-frequency components capture fine details, while low-frequency components encode global position, enabling the model to "see" the continuous geometry.}}{22}{figure.caption.6}\protected@file@percent }
\newlabel{fig:2d-pe}{{2.3}{22}{Visualization of 2D Sinusoidal Positional Encodings. High-frequency components capture fine details, while low-frequency components encode global position, enabling the model to "see" the continuous geometry}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}PEFT Methods Overview}{22}{subsection.2.5.1}\protected@file@percent }
\citation{wu2023iconshop}
\citation{rodriguez2023starvector}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Vector Graphics Generation and Understanding}{23}{section.2.6}\protected@file@percent }
\newlabel{sec:vector-graphics-generation}{{2.6}{23}{Vector Graphics Generation and Understanding}{section.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Overview of Autoregressive SVG Generation (IconShop). The model predicts the next drawing command based on the history of previous commands and a text condition. (Source: Wu et al., 2023)}}{23}{figure.caption.7}\protected@file@percent }
\newlabel{fig:iconshop-arch}{{2.4}{23}{Overview of Autoregressive SVG Generation (IconShop). The model predicts the next drawing command based on the history of previous commands and a text condition. (Source: Wu et al., 2023)}{figure.caption.7}{}}
\citation{zini2025vhector}
\citation{jain2023vectorfusion}
\citation{xing2024svgdreamer}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces StarVector Architecture. A dual-encoder system that processes both the raw SVG code and the rendered image to guide generation. (Source: Rodriguez et al., 2023)}}{24}{figure.caption.8}\protected@file@percent }
\newlabel{fig:starvector-arch}{{2.5}{24}{StarVector Architecture. A dual-encoder system that processes both the raw SVG code and the rendered image to guide generation. (Source: Rodriguez et al., 2023)}{figure.caption.8}{}}
\citation{carlier2020deepsvg}
\citation{hu2024supersvg}
\citation{ma2022live}
\citation{li2020differentiable}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces VectorFusion Pipeline. A set of random paths is optimized via Score Distillation Sampling (SDS) from a frozen text-to-image diffusion model. The differentiable rasterizer allows gradients to flow back to the vector parameters. (Source: Jain et al., 2023)}}{25}{figure.caption.9}\protected@file@percent }
\newlabel{fig:vectorfusion-arch}{{2.6}{25}{VectorFusion Pipeline. A set of random paths is optimized via Score Distillation Sampling (SDS) from a frozen text-to-image diffusion model. The differentiable rasterizer allows gradients to flow back to the vector parameters. (Source: Jain et al., 2023)}{figure.caption.9}{}}
\citation{song2023clipvg}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{27}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:metodologia}{{3}{27}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview of the Proposed Approach}{27}{section.3.1}\protected@file@percent }
\newlabel{sec:methodology-overview}{{3.1}{27}{Overview of the Proposed Approach}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Naive Approach: Direct Text Input}{27}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The Naive Approach: Direct SVG Text Input. The tokenizer fragments continuous coordinates into meaningless sub-tokens, preventing the LLM from grasping the geometric structure.}}{27}{figure.caption.10}\protected@file@percent }
\newlabel{fig:naive-pipeline}{{3.1}{27}{The Naive Approach: Direct SVG Text Input. The tokenizer fragments continuous coordinates into meaningless sub-tokens, preventing the LLM from grasping the geometric structure}{figure.caption.10}{}}
\citation{zini2025vhector}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Intermediate Approach: Text-Only Fine-Tuning. The model is adapted via LoRA to the SVG text format. While it learns to generate valid syntax, it still suffers from the tokenization mismatch and lacks true geometric perception.}}{28}{figure.caption.11}\protected@file@percent }
\newlabel{fig:text-ft-pipeline}{{3.2}{28}{Intermediate Approach: Text-Only Fine-Tuning. The model is adapted via LoRA to the SVG text format. While it learns to generate valid syntax, it still suffers from the tokenization mismatch and lacks true geometric perception}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Multimodal Training Process (SPE-LoRA)}{28}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}High-Level Pipeline}{29}{subsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Conceptual overview of the proposed SVG captioning pipeline: (1) Preprocessing, (2) SPE Encoding, (3) LoRA-adapted LLM Processing, (4) Caption Generation.}}{29}{figure.caption.12}\protected@file@percent }
\newlabel{fig:pipeline-overview}{{3.3}{29}{Conceptual overview of the proposed SVG captioning pipeline: (1) Preprocessing, (2) SPE Encoding, (3) LoRA-adapted LLM Processing, (4) Caption Generation}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Design Philosophy}{29}{subsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Role of Large Language Models in Captioning}{30}{section.3.2}\protected@file@percent }
\newlabel{sec:llms-methodology}{{3.2}{30}{The Role of Large Language Models in Captioning}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Why Decoder-Only Architectures?}{30}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Selection of LLM Backbones}{30}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Zero-Shot Baseline: Why Direct SVG Code Input Fails}{31}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Parameter-Efficient Adaptation via LoRA}{31}{section.3.3}\protected@file@percent }
\newlabel{sec:lora-methodology}{{3.3}{31}{Parameter-Efficient Adaptation via LoRA}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}The Need for Fine-Tuning}{31}{subsection.3.3.1}\protected@file@percent }
\citation{hu2021lora}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Low-Rank Adaptation: Mathematical Foundation}{32}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Target Modules and Training Strategy}{32}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Geometric Intuition: Why LoRA Works for Continuous Features}{33}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Embedding Space Architecture}{33}{section.3.4}\protected@file@percent }
\newlabel{sec:embedding-architecture}{{3.4}{33}{Embedding Space Architecture}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Dimensionality Choices}{33}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Linearization of SVG Scene Graph}{34}{subsection.3.4.2}\protected@file@percent }
\newlabel{sec:linearization}{{3.4.2}{34}{Linearization of SVG Scene Graph}{subsection.3.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Standardization Protocol}{34}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Concatenation with Text Tokens}{35}{subsection.3.4.3}\protected@file@percent }
\citation{zini2025spe_autoencoder}
\citation{zini2025vhector}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Concatenation Mechanism. The continuous visual embeddings (green) are prepended to the discrete text embeddings (blue), forming a unified sequence for the transformer. This explicitly shows the "Visual Prefix" strategy.}}{36}{figure.caption.14}\protected@file@percent }
\newlabel{fig:concatenation}{{3.4}{36}{Concatenation Mechanism. The continuous visual embeddings (green) are prepended to the discrete text embeddings (blue), forming a unified sequence for the transformer. This explicitly shows the "Visual Prefix" strategy}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Integration of the SVG Path Embedder}{36}{section.3.5}\protected@file@percent }
\newlabel{sec:spe-methodology}{{3.5}{36}{Integration of the SVG Path Embedder}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Discrete Path Embedding Strategy}{36}{subsection.3.5.1}\protected@file@percent }
\newlabel{sec:spe-discrete-strategy}{{3.5.1}{36}{Discrete Path Embedding Strategy}{subsection.3.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Detailed Architecture of the SVG Path Embedder (SPE). In alignment with the underlying code, the system processes discrete tokens rather than raw coordinates. The tokens pass through a shared embedding layer, are augmented with 1D sinusoidal positional encodings, and processed by a standard Transformer Encoder.}}{37}{figure.caption.15}\protected@file@percent }
\newlabel{fig:spe-detailed}{{3.5}{37}{Detailed Architecture of the SVG Path Embedder (SPE). In alignment with the underlying code, the system processes discrete tokens rather than raw coordinates. The tokens pass through a shared embedding layer, are augmented with 1D sinusoidal positional encodings, and processed by a standard Transformer Encoder}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Type and Style Embeddings}{37}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Multi-Layer Perceptron and Aggregation}{38}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Projection and Normalization}{38}{subsection.3.5.4}\protected@file@percent }
\citation{zini2025spe_autoencoder}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Proposed Solution: Vector-Native Processing (SPE + LoRA)}{39}{section.3.6}\protected@file@percent }
\newlabel{sec:proposed-solution-spe-lora}{{3.6}{39}{Proposed Solution: Vector-Native Processing (SPE + LoRA)}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}The SVG Path Embedder (SPE)}{39}{subsection.3.6.1}\protected@file@percent }
\newlabel{sec:spe-design}{{3.6.1}{39}{The SVG Path Embedder (SPE)}{subsection.3.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture: From Discrete Tokens to Continuous Latent Space}{39}{section*.16}\protected@file@percent }
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsubsection}{Utilization for Captioning}{40}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Positional Encoding}{40}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Integration with the LLM}{40}{subsection.3.6.2}\protected@file@percent }
\newlabel{sec:lora-integration}{{3.6.2}{40}{Integration with the LLM}{subsection.3.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Context Window and Sequence Length}{41}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Why SPE + LoRA?}{41}{subsection.3.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.5}Trade-offs: Completeness vs. Context Length}{41}{subsection.3.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Context Window Optimization}{42}{section.3.7}\protected@file@percent }
\newlabel{sec:context-optimization}{{3.7}{42}{Context Window Optimization}{section.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}The Problem: SVG Verbosity}{42}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Path Simplification and Filtering}{42}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Length-Based Filtering}{42}{subsection.3.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.4}Trade-offs: Completeness vs. Context Length}{43}{subsection.3.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Training Objective and Loss Function}{43}{section.3.8}\protected@file@percent }
\newlabel{sec:training-objective}{{3.8}{43}{Training Objective and Loss Function}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Training Objective: Autoregressive Language Modeling}{43}{subsection.3.8.1}\protected@file@percent }
\citation{hessel2021clipscore}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Masking Strategy}{44}{subsection.3.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Evaluation Metrics}{44}{section.3.9}\protected@file@percent }
\newlabel{sec:evaluation-metrics-methodology}{{3.9}{44}{Evaluation Metrics}{section.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Visual Alignment: CLIPScore}{44}{subsection.3.9.1}\protected@file@percent }
\citation{papineni2002bleu}
\citation{banerjee2005meteor}
\citation{lin2004rouge}
\bibstyle{plain}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}Linguistic Quality and Composite Metrics}{45}{subsection.3.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Summary of Methodological Choices}{45}{section.3.10}\protected@file@percent }
\newlabel{sec:methodology-summary}{{3.10}{45}{Summary of Methodological Choices}{section.3.10}{}}
\bibcite{qwen2023}{1}
\bibcite{banerjee2005meteor}{2}
\bibcite{brown2020language}{3}
\bibcite{carlier2020deepsvg}{4}
\bibcite{devlin2018bert}{5}
\bibcite{dosovitskiy2020image}{6}
\bibcite{hessel2021clipscore}{7}
\bibcite{houlsby2019parameter}{8}
\bibcite{hu2021lora}{9}
\bibcite{hu2024supersvg}{10}
\bibcite{jain2023vectorfusion}{11}
\bibcite{kaplan2020scaling}{12}
\bibcite{li2023blip2}{13}
\bibcite{li2020differentiable}{14}
\bibcite{li2021prefix}{15}
\bibcite{lin2004rouge}{16}
\bibcite{liu2023llava}{17}
\bibcite{ma2022live}{18}
\bibcite{mildenhall2020nerf}{19}
\bibcite{papineni2002bleu}{20}
\bibcite{radford2021learning}{21}
\bibcite{radford2018improving}{22}
\bibcite{rahaman2019spectral}{23}
\bibcite{rodriguez2023starvector}{24}
\bibcite{song2023clipvg}{25}
\bibcite{gemma2024}{26}
\bibcite{touvron2023llama}{27}
\bibcite{vaswani2017attention}{28}
\bibcite{w3c2011svg}{29}
\bibcite{wu2023iconshop}{30}
\bibcite{xing2024svgdreamer}{31}
\bibcite{zhang2019root}{32}
\bibcite{zini2025scalable}{33}
\bibcite{zini2025vhector}{34}
\gdef \@abspage@last{49}
