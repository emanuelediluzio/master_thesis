\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{5}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Sommario}{6}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduzione}{{1}{7}{Introduction}{chapter.1}{}}
\citation{w3c2011svg}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Challenge of Vector Graphics Captioning}{8}{section.1.1}\protected@file@percent }
\newlabel{sec:challenge-vector-captioning}{{1.1}{8}{The Challenge of Vector Graphics Captioning}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Accessibility, Search, and the Need for Automatic Captions}{9}{section.1.2}\protected@file@percent }
\newlabel{sec:motivation-accessibility}{{1.2}{9}{Accessibility, Search, and the Need for Automatic Captions}{section.1.2}{}}
\citation{zini2025vhector}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Research Hypothesis: Treating SVG as a Structured Language}{10}{section.1.3}\protected@file@percent }
\newlabel{sec:hypothesis}{{1.3}{10}{Research Hypothesis: Treating SVG as a Structured Language}{section.1.3}{}}
\citation{hu2021lora}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}The Role of Large Language Models in the Proposed Approach}{11}{section.1.4}\protected@file@percent }
\newlabel{sec:llms-role}{{1.4}{11}{The Role of Large Language Models in the Proposed Approach}{section.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Parameter-Efficient Fine-Tuning with LoRA}{11}{section.1.5}\protected@file@percent }
\newlabel{sec:peft-intro}{{1.5}{11}{Parameter-Efficient Fine-Tuning with LoRA}{section.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}The Spatial Position Encoder: Bridging Geometry and Language}{12}{section.1.6}\protected@file@percent }
\newlabel{sec:spe-intro}{{1.6}{12}{The Spatial Position Encoder: Bridging Geometry and Language}{section.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Research Objectives}{13}{section.1.7}\protected@file@percent }
\newlabel{sec:research-objectives}{{1.7}{13}{Research Objectives}{section.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Summary of Methodology and Key Findings}{13}{section.1.8}\protected@file@percent }
\newlabel{sec:summary-methodology}{{1.8}{13}{Summary of Methodology and Key Findings}{section.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Thesis Organization}{14}{section.1.9}\protected@file@percent }
\newlabel{sec:thesis-organization}{{1.9}{14}{Thesis Organization}{section.1.9}{}}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{17}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:stato-arte}{{2}{17}{State of the Art}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Foundations: Transformers and Self-Attention}{17}{section.2.1}\protected@file@percent }
\newlabel{sec:dl-foundations}{{2.1}{17}{Foundations: Transformers and Self-Attention}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Transformer Architecture}{17}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Transformer Architecture (Vaswani et al., 2017).}}{18}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:transformer}{{2.1}{18}{The Transformer Architecture (Vaswani et al., 2017)}{figure.caption.4}{}}
\citation{zhang2019root}
\citation{devlin2018bert}
\citation{radford2018improving,brown2020language}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}SVG and the Tokenization Challenge}{19}{subsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visual comparison between Vector (left) and Raster (right) graphics. Vector graphics maintain infinite resolution, while raster images pixelate when zoomed.}}{19}{figure.caption.5}\protected@file@percent }
\newlabel{fig:vector-vs-raster}{{2.2}{19}{Visual comparison between Vector (left) and Raster (right) graphics. Vector graphics maintain infinite resolution, while raster images pixelate when zoomed}{figure.caption.5}{}}
\citation{touvron2023llama}
\citation{qwen2023}
\citation{gemma2024}
\citation{kaplan2020scaling}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Large Language Models: Evolution to Decoder-Only Architectures}{20}{section.2.2}\protected@file@percent }
\newlabel{sec:llms-evolution}{{2.2}{20}{Large Language Models: Evolution to Decoder-Only Architectures}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}From BERT to GPT: Encoder vs. Decoder}{20}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Modern Decoder-Only LLMs}{20}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Why Decoder-Only for SVG Captioning?}{20}{subsection.2.2.3}\protected@file@percent }
\citation{dosovitskiy2020image}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Vision-Language Models and Their Limitations}{21}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Vision Transformers (ViT)}{21}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The Vision Transformer (ViT) Architecture. The image is split into patches, linearly projected, and processed by a standard Transformer encoder. (Source: Dosovitskiy et al., 2020)}}{21}{figure.caption.6}\protected@file@percent }
\newlabel{fig:vit-architecture}{{2.3}{21}{The Vision Transformer (ViT) Architecture. The image is split into patches, linearly projected, and processed by a standard Transformer encoder. (Source: Dosovitskiy et al., 2020)}{figure.caption.6}{}}
\citation{radford2021learning}
\citation{li2023blip2}
\citation{liu2023llava}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}CLIP: Contrastive Language-Image Pre-training}{22}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Architecture of CLIP (Contrastive Language-Image Pre-training). The model learns to align raster images and text in a shared embedding space, enabling zero-shot transfer to downstream tasks.}}{22}{figure.caption.7}\protected@file@percent }
\newlabel{fig:clip-architecture}{{2.4}{22}{Architecture of CLIP (Contrastive Language-Image Pre-training). The model learns to align raster images and text in a shared embedding space, enabling zero-shot transfer to downstream tasks}{figure.caption.7}{}}
\citation{vaswani2017attention}
\citation{rahaman2019spectral}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Generative Captioning: BLIP-2 and LLaVA}{23}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}The Vector Graphics Gap}{23}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Positional and Coordinate Encodings}{23}{section.2.4}\protected@file@percent }
\newlabel{sec:positional-encodings}{{2.4}{23}{Positional and Coordinate Encodings}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Sinusoidal Encodings: A Fourier Perspective}{23}{subsection.2.4.1}\protected@file@percent }
\newlabel{eq:sinusoidal-encoding}{{2.1}{23}{Sinusoidal Encodings: A Fourier Perspective}{equation.2.1}{}}
\citation{houlsby2019parameter}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Application to SVG Coordinates}{24}{subsection.2.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualization of 2D Sinusoidal Positional Encodings. High-frequency components capture fine details, while low-frequency components encode global position, enabling the model to "see" the continuous geometry.}}{24}{figure.caption.8}\protected@file@percent }
\newlabel{fig:2d-pe}{{2.5}{24}{Visualization of 2D Sinusoidal Positional Encodings. High-frequency components capture fine details, while low-frequency components encode global position, enabling the model to "see" the continuous geometry}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Parameter-Efficient Fine-Tuning (PEFT)}{24}{section.2.5}\protected@file@percent }
\newlabel{sec:peft-methods}{{2.5}{24}{Parameter-Efficient Fine-Tuning (PEFT)}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}The Problem: Cost and Forgetting}{24}{subsection.2.5.1}\protected@file@percent }
\citation{li2021prefix}
\citation{hu2021lora}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}PEFT Methods Overview}{25}{subsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Low-Rank Adaptation (LoRA). During training, the pre-trained weights $W_0$ are frozen, and only the low-rank matrices $A$ and $B$ are updated. At inference, they are merged back into the main weights.}}{25}{figure.caption.9}\protected@file@percent }
\newlabel{fig:lora-diagram}{{2.6}{25}{Low-Rank Adaptation (LoRA). During training, the pre-trained weights $W_0$ are frozen, and only the low-rank matrices $A$ and $B$ are updated. At inference, they are merged back into the main weights}{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Comparison of PEFT Methods}}{25}{table.caption.10}\protected@file@percent }
\newlabel{tab:peft-comparison}{{2.1}{25}{Comparison of PEFT Methods}{table.caption.10}{}}
\citation{wu2023iconshop}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Vector Graphics Generation and Understanding}{26}{section.2.6}\protected@file@percent }
\newlabel{sec:vector-graphics-generation}{{2.6}{26}{Vector Graphics Generation and Understanding}{section.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Autoregressive and LLM-Based Approaches}{26}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{IconShop and Text-Conditioned Generation}{26}{section*.11}\protected@file@percent }
\citation{rodriguez2023starvector}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Overview of Autoregressive SVG Generation (IconShop). The model predicts the next drawing command based on the history of previous commands and a text condition. (Source: Wu et al., 2023)}}{27}{figure.caption.12}\protected@file@percent }
\newlabel{fig:iconshop-arch}{{2.7}{27}{Overview of Autoregressive SVG Generation (IconShop). The model predicts the next drawing command based on the history of previous commands and a text condition. (Source: Wu et al., 2023)}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{StarVector: Integrating Code and Visuals}{27}{section*.13}\protected@file@percent }
\citation{zini2025vhector}
\citation{jain2023vectorfusion}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces StarVector Architecture. A dual-encoder system that processes both the raw SVG code and the rendered image to guide generation. (Source: Rodriguez et al., 2023)}}{28}{figure.caption.14}\protected@file@percent }
\newlabel{fig:starvector-arch}{{2.8}{28}{StarVector Architecture. A dual-encoder system that processes both the raw SVG code and the rendered image to guide generation. (Source: Rodriguez et al., 2023)}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{vHector: Bridging Geometry and Language}{28}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Diffusion-Based Methods}{28}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{VectorFusion: Score Distillation Sampling}{28}{section*.16}\protected@file@percent }
\citation{xing2024svgdreamer}
\citation{carlier2020deepsvg}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces VectorFusion Pipeline. A set of random paths is optimized via Score Distillation Sampling (SDS) from a frozen text-to-image diffusion model. The differentiable rasterizer allows gradients to flow back to the vector parameters. (Source: Jain et al., 2023)}}{29}{figure.caption.17}\protected@file@percent }
\newlabel{fig:vectorfusion-arch}{{2.9}{29}{VectorFusion Pipeline. A set of random paths is optimized via Score Distillation Sampling (SDS) from a frozen text-to-image diffusion model. The differentiable rasterizer allows gradients to flow back to the vector parameters. (Source: Jain et al., 2023)}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{SVGDreamer}{29}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Representation Learning with Transformers}{29}{subsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{DeepSVG: Hierarchical Generative Networks}{29}{section*.19}\protected@file@percent }
\citation{hu2024supersvg}
\citation{ma2022live}
\citation{li2020differentiable}
\citation{song2023clipvg}
\@writefile{toc}{\contentsline {subsubsection}{SuperSVG and Superpixel Decomposition}{30}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Parameter Optimization Approaches}{30}{subsection.2.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{LIVE and Differentiable Rasterization}{30}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ClipVG}{30}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}The Gap: From Generation to Captioning}{30}{subsection.2.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Summary}{31}{section.2.7}\protected@file@percent }
\newlabel{sec:sota-summary}{{2.7}{31}{Summary}{section.2.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{33}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:metodologia}{{3}{33}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview of the Proposed Approach}{33}{section.3.1}\protected@file@percent }
\newlabel{sec:methodology-overview}{{3.1}{33}{Overview of the Proposed Approach}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Naive Approach: Direct Text Input}{33}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The Naive Approach: Direct SVG Text Input. The tokenizer fragments continuous coordinates into meaningless sub-tokens, preventing the LLM from grasping the geometric structure.}}{33}{figure.caption.23}\protected@file@percent }
\newlabel{fig:naive-pipeline}{{3.1}{33}{The Naive Approach: Direct SVG Text Input. The tokenizer fragments continuous coordinates into meaningless sub-tokens, preventing the LLM from grasping the geometric structure}{figure.caption.23}{}}
\citation{zini2025vhector}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Baseline Training Process (LoRA-LLM)}{34}{subsection.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Intermediate Approach: Text-Only Fine-Tuning. The model is adapted via LoRA to the SVG text format. While it learns to generate valid syntax, it still suffers from the tokenization mismatch and lacks true geometric perception.}}{34}{figure.caption.24}\protected@file@percent }
\newlabel{fig:text-ft-pipeline}{{3.2}{34}{Intermediate Approach: Text-Only Fine-Tuning. The model is adapted via LoRA to the SVG text format. While it learns to generate valid syntax, it still suffers from the tokenization mismatch and lacks true geometric perception}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Multimodal Training Process (SPE-LoRA)}{34}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}High-Level Pipeline}{34}{subsection.3.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Conceptual overview of the proposed SVG captioning pipeline: (1) Preprocessing, (2) SPE Encoding, (3) LoRA-adapted LLM Processing, (4) Caption Generation.}}{35}{figure.caption.25}\protected@file@percent }
\newlabel{fig:pipeline-overview}{{3.3}{35}{Conceptual overview of the proposed SVG captioning pipeline: (1) Preprocessing, (2) SPE Encoding, (3) LoRA-adapted LLM Processing, (4) Caption Generation}{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Visualization of Path Simplification. The raw path (left) contains redundant vertices that do not contribute significantly to the shape. Filtering segments shorter than $\epsilon $ (right) reduces the token count while preserving the overall geometry.}}{35}{figure.caption.26}\protected@file@percent }
\newlabel{fig:path-simplification}{{3.4}{35}{Visualization of Path Simplification. The raw path (left) contains redundant vertices that do not contribute significantly to the shape. Filtering segments shorter than $\epsilon $ (right) reduces the token count while preserving the overall geometry}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Hybrid Sequence Construction. The continuous visual embeddings $v_i$ generated by the SPE are concatenated with the discrete text token embeddings $t_i$ to form a unified input sequence for the LLM.}}{36}{figure.caption.27}\protected@file@percent }
\newlabel{fig:hybrid-sequence}{{3.5}{36}{Hybrid Sequence Construction. The continuous visual embeddings $v_i$ generated by the SPE are concatenated with the discrete text token embeddings $t_i$ to form a unified input sequence for the LLM}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Design Philosophy}{36}{subsection.3.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Role of Large Language Models in Captioning}{37}{section.3.2}\protected@file@percent }
\newlabel{sec:llms-methodology}{{3.2}{37}{The Role of Large Language Models in Captioning}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Why Decoder-Only Architectures?}{37}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Selection of LLM Backbones}{37}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Zero-Shot Baseline: Why Direct SVG Code Input Fails}{38}{subsection.3.2.3}\protected@file@percent }
\citation{hu2021lora}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Parameter-Efficient Adaptation via LoRA}{39}{section.3.3}\protected@file@percent }
\newlabel{sec:lora-methodology}{{3.3}{39}{Parameter-Efficient Adaptation via LoRA}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}The Need for Fine-Tuning}{39}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Low-Rank Adaptation: Mathematical Foundation}{40}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Target Modules and Training Strategy}{40}{subsection.3.3.3}\protected@file@percent }
\citation{zini2025vhector}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Geometric Intuition: Why LoRA Works for Continuous Features}{41}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Integration of the Spatial Position Encoder}{41}{section.3.4}\protected@file@percent }
\newlabel{sec:spe-methodology}{{3.4}{41}{Integration of the Spatial Position Encoder}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Design Requirements}{41}{subsection.3.4.1}\protected@file@percent }
\citation{mildenhall2020nerf}
\citation{rahaman2019spectral}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Sinusoidal Coordinate Encoding}{42}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Type and Style Embeddings}{43}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Multi-Layer Perceptron and Aggregation}{43}{subsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Projection and Normalization}{43}{subsection.3.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Detailed Architecture of the Spatial Position Encoder (SPE). Continuous coordinates $(x, y)$ are mapped to high-dimensional vectors via sinusoidal functions. These are concatenated with learned embeddings for element type (e.g., circle, path) and style attributes (e.g., color), then projected and normalized to match the LLM's embedding space.}}{45}{figure.caption.28}\protected@file@percent }
\newlabel{fig:spe-detailed}{{3.6}{45}{Detailed Architecture of the Spatial Position Encoder (SPE). Continuous coordinates $(x, y)$ are mapped to high-dimensional vectors via sinusoidal functions. These are concatenated with learned embeddings for element type (e.g., circle, path) and style attributes (e.g., color), then projected and normalized to match the LLM's embedding space}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Embedding Space Architecture}{46}{section.3.5}\protected@file@percent }
\newlabel{sec:embedding-architecture}{{3.5}{46}{Embedding Space Architecture}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Dimensionality Choices}{46}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Linearization of SVG Scene Graph}{46}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Concatenation with Text Tokens}{46}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Attention Mechanism Over Hybrid Sequence}{47}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Context Window Optimization}{47}{section.3.6}\protected@file@percent }
\newlabel{sec:context-optimization}{{3.6}{47}{Context Window Optimization}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}The Problem: SVG Verbosity}{47}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Path Simplification and Filtering}{48}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Length-Based Filtering}{48}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Trade-offs: Completeness vs. Context Length}{49}{subsection.3.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Training Objective and Loss Function}{49}{section.3.7}\protected@file@percent }
\newlabel{sec:training-objective}{{3.7}{49}{Training Objective and Loss Function}{section.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Autoregressive Language Modeling}{49}{subsection.3.7.1}\protected@file@percent }
\citation{hessel2021clipscore}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Cross-Entropy Loss}{50}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Masking Strategy}{50}{subsection.3.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Evaluation Metrics}{50}{section.3.8}\protected@file@percent }
\newlabel{sec:evaluation-metrics-methodology}{{3.8}{50}{Evaluation Metrics}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Visual Alignment: CLIPScore}{50}{subsection.3.8.1}\protected@file@percent }
\citation{papineni2002bleu}
\citation{banerjee2005meteor}
\citation{lin2004rouge}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Linguistic Quality Metrics}{51}{subsection.3.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.3}Composite Score}{51}{subsection.3.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Summary of Methodological Choices}{51}{section.3.9}\protected@file@percent }
\newlabel{sec:methodology-summary}{{3.9}{51}{Summary of Methodological Choices}{section.3.9}{}}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{qwen2023}{1}
\bibcite{banerjee2005meteor}{2}
\bibcite{brown2020language}{3}
\bibcite{carlier2020deepsvg}{4}
\bibcite{devlin2018bert}{5}
\bibcite{dosovitskiy2020image}{6}
\bibcite{hessel2021clipscore}{7}
\bibcite{houlsby2019parameter}{8}
\bibcite{hu2021lora}{9}
\bibcite{hu2024supersvg}{10}
\bibcite{jain2023vectorfusion}{11}
\bibcite{kaplan2020scaling}{12}
\bibcite{li2023blip2}{13}
\bibcite{li2020differentiable}{14}
\bibcite{li2021prefix}{15}
\bibcite{lin2004rouge}{16}
\bibcite{liu2023llava}{17}
\bibcite{ma2022live}{18}
\bibcite{mildenhall2020nerf}{19}
\bibcite{papineni2002bleu}{20}
\bibcite{radford2021learning}{21}
\bibcite{radford2018improving}{22}
\bibcite{rahaman2019spectral}{23}
\bibcite{rodriguez2023starvector}{24}
\bibcite{song2023clipvg}{25}
\bibcite{gemma2024}{26}
\bibcite{touvron2023llama}{27}
\bibcite{vaswani2017attention}{28}
\bibcite{w3c2011svg}{29}
\bibcite{wu2023iconshop}{30}
\bibcite{xing2024svgdreamer}{31}
\bibcite{zhang2019root}{32}
\bibcite{zini2025vhector}{33}
\gdef \@abspage@last{56}
