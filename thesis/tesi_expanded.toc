\babel@toc {english}{}\relax 
\contentsline {chapter}{Abstract}{5}{chapter*.2}%
\contentsline {chapter}{Sommario}{6}{chapter*.3}%
\contentsline {chapter}{\numberline {1}Introduction}{8}{chapter.1}%
\contentsline {section}{\numberline {1.1}Motivation and Context}{9}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}The Accessibility Gap}{9}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Design Automation and Retrieval}{9}{subsection.1.1.2}%
\contentsline {section}{\numberline {1.2}Problem Statement: The Tokenization Bottleneck}{10}{section.1.2}%
\contentsline {section}{\numberline {1.3}Research Objectives}{10}{section.1.3}%
\contentsline {section}{\numberline {1.4}Original Contributions}{11}{section.1.4}%
\contentsline {section}{\numberline {1.5}Overview of Methodology and Results}{12}{section.1.5}%
\contentsline {section}{\numberline {1.6}Thesis Structure}{12}{section.1.6}%
\contentsline {chapter}{\numberline {2}State of the Art}{14}{chapter.2}%
\contentsline {section}{\numberline {2.1}Theoretical Foundations}{14}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}The Transformer Architecture}{14}{subsection.2.1.1}%
\contentsline {subsubsection}{Scaled Dot-Product Attention}{14}{section*.4}%
\contentsline {subsubsection}{Multi-Head Attention}{15}{section*.5}%
\contentsline {subsection}{\numberline {2.1.2}Positional Encodings in NLP}{15}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Vision Transformers (ViT)}{16}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2}Vision-Language Models (VLMs)}{16}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Contrastive Learning (CLIP)}{16}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Generative VLMs (BLIP, LLaVA)}{16}{subsection.2.2.2}%
\contentsline {section}{\numberline {2.3}Vector Graphics Understanding}{17}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Raster-based Approaches}{17}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Generative Vector Models}{17}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}The Tokenization Bottleneck}{17}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Parameter-Efficient Fine-Tuning (PEFT)}{18}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Adapters and Prefix Tuning}{18}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Low-Rank Adaptation (LoRA)}{18}{subsection.2.4.2}%
\contentsline {chapter}{\numberline {3}Methodology}{20}{chapter.3}%
\contentsline {section}{\numberline {3.1}Design Philosophy: Geometry as Language}{20}{section.3.1}%
\contentsline {section}{\numberline {3.2}The Challenge of Continuous Coordinates}{20}{section.3.2}%
\contentsline {section}{\numberline {3.3}Proposed Solution: Spatial Position Encoder (SPE)}{21}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Mathematical Formulation}{21}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Why Fourier Features?}{21}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Embedding Space Design}{22}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Alignment and Normalization}{22}{subsection.3.4.1}%
\contentsline {section}{\numberline {3.5}Fine-Tuning Strategy: LoRA}{22}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Low-Rank Adaptation (LoRA)}{23}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Hyperparameters}{23}{subsection.3.5.2}%
\contentsline {section}{\numberline {3.6}Context Window Optimization}{23}{section.3.6}%
\contentsline {chapter}{\numberline {4}Implementation}{25}{chapter.4}%
\contentsline {section}{\numberline {4.1}System Architecture}{25}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}SVG Preprocessing Module}{25}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Spatial Position Encoder (SPE) Module}{26}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}LLM Adapter and Checkpoints}{26}{subsection.4.1.3}%
\contentsline {section}{\numberline {4.2}Training Workflow}{27}{section.4.2}%
\contentsline {section}{\numberline {4.3}Embedding Space Construction}{27}{section.4.3}%
\contentsline {section}{\numberline {4.4}Our Contribution}{28}{section.4.4}%
\contentsline {chapter}{\numberline {5}Experimental Evaluation}{29}{chapter.5}%
\contentsline {section}{\numberline {5.1}Experimental Setup}{29}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Dataset and Protocol}{29}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Metrics}{29}{subsection.5.1.2}%
\contentsline {section}{\numberline {5.2}Quantitative Results}{30}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Analysis of Results}{30}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Zero-Shot Vector Performance}{31}{subsection.5.2.2}%
\contentsline {section}{\numberline {5.3}Qualitative Analysis}{32}{section.5.3}%
\contentsline {chapter}{\numberline {6}Conclusions and Future Work}{35}{chapter.6}%
\contentsline {section}{\numberline {6.1}Summary of Contributions}{35}{section.6.1}%
\contentsline {section}{\numberline {6.2}Limitations}{36}{section.6.2}%
\contentsline {section}{\numberline {6.3}Future Work}{36}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}End-to-End Generation}{36}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Hierarchical Encoders}{37}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Multimodal Pre-training}{37}{subsection.6.3.3}%
\contentsline {subsection}{\numberline {6.3.4}Interactive Editing}{37}{subsection.6.3.4}%
\contentsline {subsection}{\numberline {6.3.5}Cross-Domain Transfer}{37}{subsection.6.3.5}%
