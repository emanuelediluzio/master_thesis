\babel@toc {english}{}\relax 
\contentsline {chapter}{Abstract}{5}{chapter*.2}%
\contentsline {chapter}{Sommario}{6}{chapter*.3}%
\contentsline {chapter}{\numberline {1}Introduction}{7}{chapter.1}%
\contentsline {section}{\numberline {1.1}The Challenge of Vector Graphics Captioning}{8}{section.1.1}%
\contentsline {section}{\numberline {1.2}Accessibility, Search, and the Need for Automatic Captions}{9}{section.1.2}%
\contentsline {section}{\numberline {1.3}Research Hypothesis: Treating SVG as a Structured Language}{10}{section.1.3}%
\contentsline {section}{\numberline {1.4}The Role of Large Language Models in the Proposed Approach}{11}{section.1.4}%
\contentsline {section}{\numberline {1.5}Parameter-Efficient Fine-Tuning with LoRA}{12}{section.1.5}%
\contentsline {section}{\numberline {1.6}The SVG Path Embedder: Bridging Geometry and Language}{12}{section.1.6}%
\contentsline {section}{\numberline {1.7}Research Objectives}{13}{section.1.7}%
\contentsline {section}{\numberline {1.8}Summary of Methodology and Key Findings}{13}{section.1.8}%
\contentsline {section}{\numberline {1.9}Thesis Organization}{14}{section.1.9}%
\contentsline {chapter}{\numberline {2}State of the Art}{16}{chapter.2}%
\contentsline {section}{\numberline {2.1}Foundations: Transformers and Self-Attention}{16}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}The Transformer Architecture}{16}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}SVG and the Tokenization Challenge}{17}{subsection.2.1.2}%
\contentsline {section}{\numberline {2.2}Large Language Models: Evolution to Decoder-Only Architectures}{18}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}From BERT to GPT: Encoder vs. Decoder}{18}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Modern Decoder-Only LLMs}{19}{subsection.2.2.2}%
\contentsline {section}{\numberline {2.3}Vision-Language Models and Their Limitations}{20}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Vision Transformers (ViT)}{20}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}CLIP: Contrastive Language-Image Pre-training}{21}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Generative Captioning: BLIP-2 and LLaVA}{22}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}The Vector Graphics Gap}{22}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Positional and Coordinate Encodings}{23}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Sinusoidal Encodings in Transformers}{23}{subsection.2.4.1}%
\contentsline {section}{\numberline {2.5}Parameter-Efficient Fine-Tuning (PEFT)}{24}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}PEFT Methods Overview}{25}{subsection.2.5.1}%
\contentsline {section}{\numberline {2.6}Vector Graphics Generation and Understanding}{25}{section.2.6}%
\contentsline {chapter}{\numberline {3}Methodology}{30}{chapter.3}%
\contentsline {section}{\numberline {3.1}Overview of the Proposed Approach}{30}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}The Naive Approach: Direct Text Input}{30}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Multimodal Training Process (SPE-LoRA)}{31}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}High-Level Pipeline}{32}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Design Philosophy}{33}{subsection.3.1.4}%
\contentsline {section}{\numberline {3.2}The Role of Large Language Models in Captioning}{33}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Why Decoder-Only Architectures?}{33}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Selection of LLM Backbones}{33}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Zero-Shot Baseline: Why Direct SVG Code Input Fails}{34}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Parameter-Efficient Adaptation via LoRA}{34}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}The Need for Fine-Tuning}{35}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Low-Rank Adaptation: Mathematical Foundation}{35}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Target Modules and Training Strategy}{35}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Geometric Intuition: Why LoRA Works for Continuous Features}{36}{subsection.3.3.4}%
\contentsline {section}{\numberline {3.4}Embedding Space Architecture}{37}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Dimensionality Choices}{37}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Standardization into SVG Path List}{37}{subsection.3.4.2}%
\contentsline {subsubsection}{Standardization Protocol}{37}{section*.16}%
\contentsline {subsection}{\numberline {3.4.3}Concatenation with Text Tokens}{38}{subsection.3.4.3}%
\contentsline {section}{\numberline {3.5}Integration of the SVG Path Embedder}{39}{section.3.5}%
\contentsline {section}{\numberline {3.6}Proposed Solution: Vector-Native Processing (SPE + LoRA)}{40}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}The SVG Path Embedder (SPE)}{40}{subsection.3.6.1}%
\contentsline {subsubsection}{Architecture: From Discrete Tokens to Continuous Latent Space}{41}{section*.19}%
\contentsline {subsubsection}{Utilization for Captioning}{41}{section*.20}%
\contentsline {subsubsection}{Positional Encoding}{41}{section*.21}%
\contentsline {subsection}{\numberline {3.6.2}Integration with the LLM}{42}{subsection.3.6.2}%
\contentsline {subsection}{\numberline {3.6.3}Context Window and Sequence Length}{42}{subsection.3.6.3}%
\contentsline {subsection}{\numberline {3.6.4}Why SPE + LoRA?}{42}{subsection.3.6.4}%
\contentsline {subsection}{\numberline {3.6.5}Trade-offs: Completeness vs. Context Length}{43}{subsection.3.6.5}%
\contentsline {section}{\numberline {3.7}Context Window Optimization}{43}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}The Problem: SVG Verbosity}{43}{subsection.3.7.1}%
\contentsline {subsection}{\numberline {3.7.2}Path Simplification and Filtering}{43}{subsection.3.7.2}%
\contentsline {subsection}{\numberline {3.7.3}Length-Based Filtering}{44}{subsection.3.7.3}%
\contentsline {subsection}{\numberline {3.7.4}Trade-offs: Completeness vs. Context Length}{44}{subsection.3.7.4}%
\contentsline {section}{\numberline {3.8}Training Objective and Loss Function}{45}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Training Objective: Autoregressive Language Modeling}{45}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}Masking Strategy}{45}{subsection.3.8.2}%
\contentsline {section}{\numberline {3.9}Evaluation Metrics}{45}{section.3.9}%
\contentsline {subsection}{\numberline {3.9.1}Visual Alignment: CLIPScore}{46}{subsection.3.9.1}%
\contentsline {subsection}{\numberline {3.9.2}Linguistic Quality and Composite Metrics}{46}{subsection.3.9.2}%
\contentsline {section}{\numberline {3.10}Summary of Methodological Choices}{47}{section.3.10}%
\contentsline {chapter}{\numberline {4}Implementation}{48}{chapter.4}%
\contentsline {section}{\numberline {4.1}System Architecture and Technology Stack}{48}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Dataset Construction and Preprocessing}{48}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}SVG Parsing and Tokenization}{49}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Dynamic Padding and Collation}{50}{subsection.4.1.3}%
\contentsline {section}{\numberline {4.2}The SVG Path Embedder (SPE)}{50}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}The SPE Module}{51}{subsection.4.2.1}%
\contentsline {section}{\numberline {4.3}LoRA Integration and Training Loop}{52}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Loading the Pre-trained LLM}{52}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Injecting LoRA Adapters}{52}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Baseline Training Process (LoRA-LLM)}{52}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}Multimodal Training Process (SPE-LoRA)}{52}{subsection.4.3.4}%
\contentsline {subsubsection}{Visual-Semantic Metric (Primary)}{53}{section*.23}%
\contentsline {section}{\numberline {4.4}Challenges and Solutions}{54}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Addressing Gradient Explosion}{54}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Hyperparameters and Training Details}{55}{subsection.4.4.2}%
\contentsline {chapter}{\numberline {5}Experimental Evaluation}{57}{chapter.5}%
\contentsline {section}{\numberline {5.1}Experimental Setup}{57}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Dataset Curation: The Icons8 Benchmark}{57}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Dataset}{57}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {5.1.3}Evaluation Metrics}{57}{subsection.5.1.3}%
\contentsline {subsubsection}{Linguistic Metrics (Reference-Based)}{58}{section*.26}%
\contentsline {subsubsection}{Visual-Semantic Metric (Reference-Free)}{58}{section*.27}%
\contentsline {subsubsection}{Composite Score}{58}{section*.28}%
\contentsline {subsection}{\numberline {5.1.4}Architectural Rationale: Why Decoder-Only?}{58}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Baselines and Models}{58}{subsection.5.1.5}%
\contentsline {section}{\numberline {5.2}Quantitative Results}{59}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Analysis of Results}{59}{subsection.5.2.1}%
\contentsline {section}{\numberline {5.3}Ablation Studies}{60}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}LoRA Rank}{60}{subsection.5.3.1}%
\contentsline {section}{\numberline {5.4}Qualitative Analysis}{60}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Success Cases}{61}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Qualitative Comparison Across Architectures}{61}{subsection.5.4.2}%
\contentsline {subsection}{\numberline {5.4.3}Failure Modes and Error Taxonomy}{64}{subsection.5.4.3}%
\contentsline {subsubsection}{1. Geometric Hallucination}{64}{section*.32}%
\contentsline {subsubsection}{2. Attribute Mismatch}{65}{section*.33}%
\contentsline {subsubsection}{3. OCR Failure}{65}{section*.34}%
\contentsline {subsubsection}{4. Abstract Concept Failure}{65}{section*.35}%
\contentsline {section}{\numberline {5.5}Discussion}{65}{section.5.5}%
\contentsline {section}{\numberline {5.6}Qualitative Analysis Gallery}{66}{section.5.6}%
\contentsline {chapter}{\numberline {6}Conclusions and Future Work}{87}{chapter.6}%
\contentsline {section}{\numberline {6.1}Summary of the Research Journey}{87}{section.6.1}%
\contentsline {section}{\numberline {6.2}Main Contributions Revisited}{88}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Empirical Contributions}{89}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Practical Contributions}{89}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Empirical Findings and Insights}{89}{section.6.3}%
\contentsline {section}{\numberline {6.4}Limitations}{89}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Technical Limitations}{89}{subsection.6.4.1}%
\contentsline {subsection}{\numberline {6.4.2}Dataset and Domain Limitations}{90}{subsection.6.4.2}%
\contentsline {subsection}{\numberline {6.4.3}Evaluation Limitations}{90}{subsection.6.4.3}%
\contentsline {section}{\numberline {6.5}Ethical Considerations and Responsible Deployment}{91}{section.6.5}%
\contentsline {section}{\numberline {6.6}Future Research Directions}{91}{section.6.6}%
\contentsline {subsection}{\numberline {6.6.1}Architectural Extensions}{91}{subsection.6.6.1}%
\contentsline {subsubsection}{Hierarchical Spatial Encoders}{91}{section*.62}%
\contentsline {subsubsection}{Multi-Scale Feature Fusion}{91}{section*.64}%
\contentsline {subsubsection}{Hybrid Raster-Vector Encoders}{91}{section*.65}%
\contentsline {subsubsection}{OCR Integration}{92}{section*.67}%
\contentsline {subsection}{\numberline {6.6.2}Dataset and Domain Expansion}{92}{subsection.6.6.2}%
\contentsline {subsubsection}{Technical Diagrams and Data Visualizations}{92}{section*.68}%
\contentsline {subsubsection}{Cross-Dataset Generalization}{93}{section*.69}%
\contentsline {subsubsection}{Multilingual Captions}{93}{section*.70}%
\contentsline {subsubsection}{Synthetic Data Generation}{94}{section*.71}%
\contentsline {subsection}{\numberline {6.6.3}Multimodal Pretraining}{94}{subsection.6.6.3}%
\contentsline {subsubsection}{SVG-Image Contrastive Learning}{94}{section*.72}%
\contentsline {subsubsection}{Joint SVG-Text-Image Pretraining}{94}{section*.73}%
\contentsline {subsubsection}{Masked SVG Modeling}{94}{section*.74}%
\contentsline {subsection}{\numberline {6.6.4}Evaluation Methodology}{94}{subsection.6.6.4}%
\contentsline {subsubsection}{Human Evaluation at Scale}{94}{section*.75}%
\contentsline {subsubsection}{User Studies with Visually Impaired Participants}{94}{section*.76}%
\contentsline {subsubsection}{Benchmarking on Standardized Suites}{95}{section*.77}%
\contentsline {subsection}{\numberline {6.6.5}Broader Vision: End-to-End Vector-Language Systems}{95}{subsection.6.6.5}%
\contentsline {section}{\numberline {6.7}Closing Remarks}{95}{section.6.7}%
\contentsline {chapter}{Acknowledgments}{97}{chapter*.78}%
